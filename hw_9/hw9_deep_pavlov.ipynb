{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "hw9_deep_pavlov.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55gcZUbxSzcp"
      },
      "source": [
        "#  Few-shot распознавание сущностей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0TFmBYSzcv"
      },
      "source": [
        "Берт и возможность тренировать few-shot ner модели есть в deep pavlov (http://docs.deeppavlov.ai/en/master/features/models/ner.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arQy5cF_U7Wp",
        "outputId": "d5d5cff0-5e70-4517-f961-8eb6fe1cf05a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #, force_remount=True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ydB0hukVDUg"
      },
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks/nlp_hw')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B7CKfNAchJ4d",
        "outputId": "b466551a-a609-4c68-ffd3-d2ecfb9654e4"
      },
      "source": [
        "!pip install deeppavlov"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/08/b0e17b109873563a78c9c06437db1bcc698592f86b5aebe5038004b5969e/deeppavlov-0.14.0-py3-none-any.whl (988kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/fc/12de89822adaa3a60b8cb0139bae75918278999d08e6dff158623abd7cba/ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 17.2MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/a4/a48bd4b0d15395362b561df7e7247de87291105eb736a3b2aaffebf437b9/scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.41.1)\n",
            "Collecting uvicorn==0.11.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 48.4MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 49.9MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n",
            "Collecting pytz==2019.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 43.0MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Collecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/58/2deb24de3c10cc4c0f09639b46f4f4b50059f0fdc785128a57dd9fdce026/Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.5MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 50.3MB/s \n",
            "\u001b[?25hCollecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/56/1f652c3f658d2a9fd495d2e988a2da57eabdb6c4b8f4563c2ccbe6a2a8c5/pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 26.2MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/53/127cb49435bcf5d841baf8eafa030931c62a9eac577a641f8c2293d23371/numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 66.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Collecting pymorphy2-dicts-ru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deeppavlov) (1.15.0)\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2->deeppavlov) (1.0.1)\n",
            "Collecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/295101ea5a60f9bee805a3ca422863600ba5cac4e2778ac7bd56efab1231/httptools-0.1.1-cp37-cp37m-manylinux1_x86_64.whl (217kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 44.7MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/05/805df4850d9659efd69d00076269ae6adcb0e151d1922cff822ead2c432a/uvloop-0.15.2-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Collecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 58.7MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/c4/dc5b9d50c15af2ee187974a5a0c3f20c06cce6559eea4c065d372e846b6a/aiormq-3.3.1-py3-none-any.whl\n",
            "Collecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.8MB/s \n",
            "\u001b[?25hCollecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n",
            "Collecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "Building wheels for collected packages: pytelegrambotapi, overrides, prometheus-client, nltk, sacremoses, starlette\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp37-none-any.whl size=47177 sha256=bf2ed06c5e5b7a9ddfbda3628034688797fba15f48019a01bb7cc7f4fea3593b\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp37-none-any.whl size=5600 sha256=c71068eaa035dec795c85a6eb9c4426b47a9c8323065e914abbb55ae4ec5b014\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-cp37-none-any.whl size=41404 sha256=cfeeed290cc21f2eebb3f5b9849979d4c9576b5449796de13f6e6525a3114223\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449906 sha256=507affff9906b701db5c987e4cbdce74c6d501f136d955224cf6daf5e3bab402\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883999 sha256=76ddbd70a77f2fae7f98de3fffd039edc87667dce66cfd492c092ae16f1ba65a\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp37-none-any.whl size=57244 sha256=2642cf0a6db0e83301b90ea64971d87bd7d481da1b58ce4bad47b195078bb967\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "Successfully built pytelegrambotapi overrides prometheus-client nltk sacremoses starlette\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pydantic, starlette, fastapi, ruamel.yaml, dawg-python, pymorphy2-dicts, pymorphy2, numpy, scikit-learn, httptools, websockets, h11, uvloop, uvicorn, idna, requests, pytelegrambotapi, overrides, pytz, pandas, prometheus-client, nltk, multidict, yarl, pamqp, aiormq, aio-pika, cryptography, pyopenssl, rusenttokenize, Cython, sacremoses, pymorphy2-dicts-ru, deeppavlov\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: prometheus-client 0.9.0\n",
            "    Uninstalling prometheus-client-0.9.0:\n",
            "      Successfully uninstalled prometheus-client-0.9.0\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: Cython 0.29.22\n",
            "    Uninstalling Cython-0.29.22:\n",
            "      Successfully uninstalled Cython-0.29.22\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-3.4.7 dawg-python-0.7.2 deeppavlov-0.14.0 fastapi-0.47.1 h11-0.9.0 httptools-0.1.1 idna-2.8 multidict-5.1.0 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7 uvloop-0.15.2 websockets-8.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "pytz"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw8bhWHJSzcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f146d752-28d1-403a-ac02-26ae3e2aa393"
      },
      "source": [
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.18.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.32.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (54.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=b957ad5cd74b2da928d5d1a8f8de31b9a65c949e968118a68755b62e6aae3223\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, keras-applications, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2LIQqoQmSzcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16c540f-bb1f-46ac-d9bb-97e61e4337f2"
      },
      "source": [
        "!apt-get --yes install git"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOtoNuwklpAG",
        "outputId": "911dba6b-6221-4bd3-b496-eaaa86b3787e"
      },
      "source": [
        "!pip install git+https://github.com/deepmipt/bert.git@feat/multi_gpu"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-h3gfdut1\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-h3gfdut1\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23581 sha256=4039a877fba67d92e586e29275f23e8ae051a8d8442224d82dc451a4babc8bb3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bahdg4yf/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Installing collected packages: bert-dp\n",
            "Successfully installed bert-dp-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-HlTJ8WWKDJ",
        "outputId": "c51ddd85-20e7-4e65-8cf0-d7587b2684d9"
      },
      "source": [
        "pip install razdel"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting razdel\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHR-g3ZNp4PT"
      },
      "source": [
        "Для удобства достанем предложения подходящего формата регуляркой и раскидаем их на три сета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5tmsongVW5D"
      },
      "source": [
        "with open('avito_cars.txt', 'r', encoding='utf-8') as f:\n",
        "  texts = f.readlines()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN784LuiV7IN"
      },
      "source": [
        "from razdel import tokenize as razdel_tokenize\n",
        "import re\n",
        "\n",
        "reSent = re.compile('[Пп]родам.*?\\.|[Пп]родаю.*?\\.|[Пп]окупаю.*?\\.|[Кк]уплю.*?\\.', re.DOTALL)\n",
        "\n",
        "def split_texts(num, file_name, texts):\n",
        "  sent_count = 0\n",
        "  f = open(file_name, 'a', encoding='utf-8')\n",
        "  for t in range(len(texts)):\n",
        "    sents = re.findall(reSent, texts[t])\n",
        "    sent_count += len(sents)\n",
        "    for i in sents:\n",
        "     words = [token.text for token in list(razdel_tokenize(i)) if token.text != '/']\n",
        "     for w in words:\n",
        "      f.write(w+'\\n')\n",
        "    f.write('\\n')\n",
        "    if sent_count == num:\n",
        "      f.close()\n",
        "      return t"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsaxL4MmVu73",
        "outputId": "b55fb95b-e4e9-4881-e0b1-58e4e601b90c"
      },
      "source": [
        "split_texts(15, 'test.txt', texts[112:])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYpG_cBFSzcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0852858-9e4e-420a-923b-7d76543525c5"
      },
      "source": [
        "!ls *.txt"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avito_cars.txt\ttest.txt  train.txt  valid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaftQr_gSzcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6724e24f-3529-4ca5-f94e-324ff243a203"
      },
      "source": [
        "!head test.txt"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Продам O\n",
            "Audi B-AUTO\n",
            "A I-AUTO\n",
            "6 I-AUTO\n",
            ", O\n",
            "объём O\n",
            "2 O\n",
            ". O\n",
            "\n",
            "Продаю O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp4Mu9igSzcz"
      },
      "source": [
        "Попробуем теперь обучить разметчик сущностей поверх многоязычного берта. Знать, что это такое и как работает не обязательно. Обязательно указать путь к обучающий файлам."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsvydQwjbHo2"
      },
      "source": [
        "import json\n",
        "from deeppavlov import configs, build_model, train_model\n",
        "\n",
        "with configs.ner.ner_ontonotes_bert_mult.open(encoding='utf8') as f:\n",
        "    ner_config = json.load(f)\n",
        "\n",
        "ner_config['dataset_reader']['data_path'] = './'  # directory with train.txt, valid.txt and test.txt files\n",
        "ner_config['metadata']['variables']['NER_PATH'] = './'\n",
        "ner_config['metadata']['download'] = [ner_config['metadata']['download'][-1]]  # do not download the pretrained ontonotes model"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPsgcoayw_iG"
      },
      "source": [
        "!!! Если колаб крашиться из-за памяти, то после перезапуска не запускайте ячейку с train_model и build_model по очереди. Либо запустите только train_model, либо пропустите ее и переходите к build_model ниже"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcLZmVPMqchZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92590d54-c217-4a7d-9853-34b1aa951cad"
      },
      "source": [
        "ner_model = train_model(ner_config, download=True)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:00:10.864 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip\n",
            "100%|██████████| 663M/663M [03:08<00:00, 3.52MB/s]\n",
            "2021-04-02 13:03:20.946 INFO in 'deeppavlov.core.data.utils'['utils'] at line 268: Extracting /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip archive into /root/.deeppavlov/downloads/bert_models\n",
            "2021-04-02 13:03:28.50 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "2021-04-02 13:03:32.519 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /content/gdrive/My Drive/Colab Notebooks/nlp_hw/tag.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:236: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:314: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:75: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:94: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:671: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:249: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:04:06.211 INFO in 'deeppavlov.models.bert.bert_sequence_tagger'['bert_sequence_tagger'] at line 251: [initializing model with Bert from /root/.deeppavlov/downloads/bert_models/multi_cased_L-12_H-768_A-12/bert_model.ckpt]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/bert/bert_sequence_tagger.py:255: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/downloads/bert_models/multi_cased_L-12_H-768_A-12/bert_model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:04:14.415 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best ner_f1 of 13.0435\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 15, \"metrics\": {\"ner_f1\": 13.0435, \"ner_token_f1\": 36.8421}, \"time_spent\": \"0:00:06\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 100}}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/trainers/nn_trainer.py:250: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"ner_f1\": 46.1538, \"ner_token_f1\": 52.1739}, \"time_spent\": \"0:04:50\", \"epochs_done\": 9, \"batches_seen\": 20, \"train_examples_seen\": 280, \"head_learning_rate\": 0.009999999776482582, \"bert_learning_rate\": 1.9999999552965164e-05, \"loss\": 7.431668293476105}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:09:02.357 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ner_f1 of 18.1818\n",
            "2021-04-02 13:09:02.359 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2021-04-02 13:09:02.500 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/gdrive/My Drive/Colab Notebooks/nlp_hw/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 15, \"metrics\": {\"ner_f1\": 18.1818, \"ner_token_f1\": 16.6667}, \"time_spent\": \"0:04:54\", \"epochs_done\": 9, \"batches_seen\": 20, \"train_examples_seen\": 280, \"impatience\": 0, \"patience_limit\": 100}}\n",
            "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"ner_f1\": 25.0, \"ner_token_f1\": 55.1724}, \"time_spent\": \"0:09:54\", \"epochs_done\": 19, \"batches_seen\": 40, \"train_examples_seen\": 560, \"head_learning_rate\": 0.009999999776482582, \"bert_learning_rate\": 1.9999999552965164e-05, \"loss\": 4.259630692005158}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:14:06.134 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ner_f1 of 18.1818\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 15, \"metrics\": {\"ner_f1\": 0, \"ner_token_f1\": 7.6923}, \"time_spent\": \"0:09:58\", \"epochs_done\": 19, \"batches_seen\": 40, \"train_examples_seen\": 560, \"impatience\": 1, \"patience_limit\": 100}}\n",
            "{\"train\": {\"eval_examples_count\": 16, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:14:38\", \"epochs_done\": 29, \"batches_seen\": 60, \"train_examples_seen\": 840, \"head_learning_rate\": 0.009999999776482582, \"bert_learning_rate\": 1.9999999552965164e-05, \"loss\": 1.2151655793190002}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:18:50.249 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ner_f1 of 72.7273\n",
            "2021-04-02 13:18:50.251 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model\n",
            "2021-04-02 13:18:50.391 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /content/gdrive/My Drive/Colab Notebooks/nlp_hw/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 15, \"metrics\": {\"ner_f1\": 72.7273, \"ner_token_f1\": 91.6667}, \"time_spent\": \"0:14:42\", \"epochs_done\": 29, \"batches_seen\": 60, \"train_examples_seen\": 840, \"impatience\": 0, \"patience_limit\": 100}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:19:02.487 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/gdrive/My Drive/Colab Notebooks/nlp_hw/tag.dict]\n",
            "2021-04-02 13:19:50.33 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/gdrive/My Drive/Colab Notebooks/nlp_hw/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/nlp_hw/model\n",
            "{\"valid\": {\"eval_examples_count\": 15, \"metrics\": {\"ner_f1\": 72.7273, \"ner_token_f1\": 91.6667}, \"time_spent\": \"0:00:06\"}}\n",
            "{\"test\": {\"eval_examples_count\": 15, \"metrics\": {\"ner_f1\": 69.5652, \"ner_token_f1\": 91.6667}, \"time_spent\": \"0:00:04\"}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 13:20:28.705 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /content/gdrive/My Drive/Colab Notebooks/nlp_hw/tag.dict]\n",
            "2021-04-02 13:20:59.328 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /content/gdrive/My Drive/Colab Notebooks/nlp_hw/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/nlp_hw/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QRFtdPbSzcz"
      },
      "source": [
        "Посмотрим как это все размечается."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWWCrWlbSzcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46ef248-b3b0-435f-dadb-f41b099267a2"
      },
      "source": [
        "ner_model(['ВАЗ 21063', 'Продам ваз 21055', 'Волгу на обмен'])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['ВАЗ', '21063'], ['Продам', 'ваз', '21055'], ['Волгу', 'на', 'обмен']],\n",
              " [['B-AUTO', 'I-AUTO'], ['O', 'B-AUTO', 'I-AUTO'], ['I-AUTO', 'O', 'O']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEdgWjUiC5q7"
      },
      "source": [
        "marked = []\n",
        "\n",
        "for text in texts[:300]:\n",
        "    if len(text.split()) > 100:\n",
        "        continue\n",
        "    pred = ner_model([text])\n",
        "    sent, tags = pred[0][0], pred[1][0]\n",
        "    \n",
        "    if len(set(tags[0])) > 1:\n",
        "        marked.append(list(zip(sent,tags)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbPmC0v1keh1",
        "outputId": "ad7b9085-2acc-4a68-cca5-7e0cd9361b46"
      },
      "source": [
        "marked[:2]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Ваз', 'B-AUTO'),\n",
              "  ('2107', 'I-AUTO'),\n",
              "  ('в', 'O'),\n",
              "  ('отличном', 'O'),\n",
              "  ('состоянии', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('полностью', 'O'),\n",
              "  ('на', 'O'),\n",
              "  ('ходу', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('коробка', 'O'),\n",
              "  ('5ст', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('Центральный', 'O'),\n",
              "  ('замок', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('музыка', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('хорошая', 'O'),\n",
              "  ('летняя', 'O'),\n",
              "  ('резина', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('торг', 'O'),\n",
              "  ('уместен', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('/', 'O'),\n",
              "  ('/', 'O'),\n",
              "  ('Ваз', 'B-AUTO'),\n",
              "  ('2109', 'I-AUTO'),\n",
              "  ('1990г', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('в', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('В', 'O'),\n",
              "  ('отличном', 'O'),\n",
              "  ('состоянии', 'O'),\n",
              "  ('на', 'O'),\n",
              "  ('полном', 'O'),\n",
              "  ('ходу', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('салон', 'O'),\n",
              "  ('ухоженный', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('передние', 'O'),\n",
              "  ('сиденья', 'O'),\n",
              "  ('на', 'O'),\n",
              "  ('пульте', 'O'),\n",
              "  ('управления', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('Центральный', 'O'),\n",
              "  ('замок', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('музыка', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('торг', 'O'),\n",
              "  ('уместен', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('52', 'B-AUTO'),\n",
              "  ('000руб', 'I-AUTO'),\n",
              "  ('.', 'O'),\n",
              "  ('\\n', 'O')],\n",
              " [('Газель', 'I-AUTO'),\n",
              "  ('в', 'O'),\n",
              "  ('отличном', 'O'),\n",
              "  ('состоянии', 'O'),\n",
              "  ('.', 'O'),\n",
              "  ('Двигатель', 'O'),\n",
              "  ('от', 'O'),\n",
              "  ('Уаза', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('был', 'O'),\n",
              "  ('сделан', 'O'),\n",
              "  ('капитальный', 'O'),\n",
              "  ('ремонт', 'O'),\n",
              "  ('двигателя', 'O'),\n",
              "  ('и', 'O'),\n",
              "  ('коробки', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('новый', 'O'),\n",
              "  ('стартер', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('генератор', 'O'),\n",
              "  (',', 'O'),\n",
              "  ('радиатор', 'O'),\n",
              "  ('(', 'O'),\n",
              "  ('трехрядный', 'O'),\n",
              "  (')', 'O'),\n",
              "  ('Новая', 'O'),\n",
              "  ('резина', 'O'),\n",
              "  ('по', 'O'),\n",
              "  ('кругу', 'O'),\n",
              "  ('(', 'O'),\n",
              "  ('бескамерная', 'O'),\n",
              "  (')', 'O'),\n",
              "  ('\\n', 'O')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6tjccPZ-7dI"
      },
      "source": [
        "В приницпе марки различаются, однако не в 100% случаях, также некоторые неподходящие слова могут размечаться, как марки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFYNsz-cSzc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6946755-b855-4a1b-86a6-dc936511e978"
      },
      "source": [
        "ner_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-02 14:29:14.891 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_mult_v1.tar.gz to /root/.deeppavlov/ner_ontonotes_bert_mult_v1.tar.gz\n",
            "100%|██████████| 1.32G/1.32G [03:19<00:00, 6.62MB/s]\n",
            "2021-04-02 14:32:35.296 INFO in 'deeppavlov.core.data.utils'['utils'] at line 268: Extracting /root/.deeppavlov/ner_ontonotes_bert_mult_v1.tar.gz archive into /root/.deeppavlov/models\n",
            "2021-04-02 14:33:05.862 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n",
            "2021-04-02 14:33:06.316 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert_mult/tag.dict]\n",
            "2021-04-02 14:33:39.157 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sevGFcAfSzc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbecf89-0f3e-471c-9c31-9bb95b05167b"
      },
      "source": [
        "ner_model(['Передай привет Михаилу Нефедову'])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Передай', 'привет', 'Михаилу', 'Нефедову']],\n",
              " [['O', 'O', 'B-PERSON', 'I-PERSON']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7a9PXTbSzc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ffb38-cc15-49ec-b135-aec4b6ff6548"
      },
      "source": [
        "ner_model(['Поезжай в Москву'])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Поезжай', 'в', 'Москву']], [['O', 'O', 'B-GPE']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap0G1DB_Szc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5dda56-6198-43e0-f636-0638f5374853"
      },
      "source": [
        "ner_model(['Поезжай в Альпы'])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Поезжай', 'в', 'Альпы']], [['O', 'O', 'B-LOC']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRuAzbdESzc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e188b5-640d-4e7c-ede7-ce9e48d9728b"
      },
      "source": [
        "ner_model(['Я говорю на русском и английском языках'])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Я', 'говорю', 'на', 'русском', 'и', 'английском', 'языках']],\n",
              " [['O', 'O', 'O', 'B-LANGUAGE', 'O', 'B-LANGUAGE', 'O']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWwM4IZXSzc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "898888fd-dacc-46d2-966c-47cd10d40461"
      },
      "source": [
        "ner_model(['Бутылка воды стоит 50 рублей'])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['Бутылка', 'воды', 'стоит', '50', 'рублей']],\n",
              " [['O', 'O', 'O', 'B-MONEY', 'I-MONEY']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msWwrHe8Szc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959a6c2c-f9bd-4b94-842d-55e8e5cb909e"
      },
      "source": [
        "ner_model(['ле Биг мак'])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['ле', 'Биг', 'мак']], [['B-PERSON', 'I-PERSON', 'I-PERSON']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou2kX4A9BSep"
      },
      "source": [
        "Разметим новые примеры из датасета и посмотрим как справится модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKEWNLwhiK-H"
      },
      "source": [
        "marked = []\n",
        "\n",
        "for text in texts[200:400]:\n",
        "    if len(text.split()) > 100:\n",
        "        continue\n",
        "    pred = ner_model([text])\n",
        "    sent, tags = pred[0][0], pred[1][0]\n",
        "    \n",
        "    if len(set(tags[0])) > 1:\n",
        "        marked.append(list(zip(sent,tags)))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siQ96XDydHPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e3ac95-80ee-455a-8a1e-750cd7ac13ce"
      },
      "source": [
        "marked[0]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('KIA', 'B-PRODUCT'),\n",
              " ('BONGO', 'I-PRODUCT'),\n",
              " ('III', 'I-PRODUCT'),\n",
              " (',', 'O'),\n",
              " ('2011', 'B-DATE'),\n",
              " ('год', 'I-DATE'),\n",
              " ('.', 'O'),\n",
              " ('4WD', 'B-CARDINAL'),\n",
              " ('.', 'O'),\n",
              " ('5MКПП', 'B-CARDINAL'),\n",
              " ('.', 'O'),\n",
              " ('Двухкабинник', 'O'),\n",
              " (',', 'O'),\n",
              " ('6', 'B-CARDINAL'),\n",
              " ('мест', 'O'),\n",
              " (',', 'O'),\n",
              " ('кондиционер', 'O'),\n",
              " (',', 'O'),\n",
              " ('противотуманные', 'O'),\n",
              " ('фары', 'O'),\n",
              " (',', 'O'),\n",
              " ('т', 'O'),\n",
              " (',', 'O'),\n",
              " ('отключаемый', 'O'),\n",
              " ('передний', 'O'),\n",
              " ('мост', 'O'),\n",
              " ('и', 'O'),\n",
              " ('пониженная', 'O'),\n",
              " ('передача', 'O'),\n",
              " ('с', 'O'),\n",
              " ('раздатки', 'O'),\n",
              " ('4H', 'B-CARDINAL'),\n",
              " ('/', 'O'),\n",
              " ('4L', 'B-CARDINAL'),\n",
              " ('/', 'O'),\n",
              " ('2', 'B-CARDINAL'),\n",
              " (',', 'O'),\n",
              " ('вал', 'O'),\n",
              " ('отбора', 'O'),\n",
              " ('мощности', 'O'),\n",
              " (',', 'O'),\n",
              " ('электрические', 'O'),\n",
              " ('стеклоподъемники', 'O'),\n",
              " (',', 'O'),\n",
              " ('подогрев', 'O'),\n",
              " ('сидений', 'O'),\n",
              " (',', 'O'),\n",
              " ('обогрев', 'O'),\n",
              " ('зеркал', 'O'),\n",
              " (',', 'O'),\n",
              " ('электрорегулировка', 'O'),\n",
              " ('зеркал', 'O'),\n",
              " (',', 'O'),\n",
              " ('центральный', 'O'),\n",
              " ('замок', 'O'),\n",
              " ('.', 'O'),\n",
              " ('/', 'O'),\n",
              " ('Защита', 'O'),\n",
              " ('картера', 'O'),\n",
              " ('двигателя', 'O'),\n",
              " ('и', 'O'),\n",
              " ('мкпп', 'O'),\n",
              " ('новый', 'O'),\n",
              " ('аккумулятор', 'O'),\n",
              " ('есть', 'O'),\n",
              " ('тент', 'O'),\n",
              " ('родной', 'O'),\n",
              " ('Один', 'B-CARDINAL'),\n",
              " ('хозяин', 'O'),\n",
              " ('автомобиль', 'O'),\n",
              " ('в', 'O'),\n",
              " ('дтп', 'O'),\n",
              " ('не', 'O'),\n",
              " ('участовал', 'O'),\n",
              " ('торг', 'O'),\n",
              " ('\\n', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpLNXMSwCALF"
      },
      "source": [
        "Неправильно размеченные примеры:\n",
        "\n",
        "1) (KIA', 'B-PRODUCT'), ('BONGO', 'I-PRODUCT'), ('III', 'I-PRODUCT'),\n",
        " --- тут правильно разметилось начало и продолжение сущности, однако не стоит тег AUTO, которым размечались марки\n",
        "\n",
        "2) ('4WD', 'B-CARDINAL') ... ('5MКПП', 'B-CARDINAL') --- я не уверена, считаются ли примеры вида \"Цифра+буква\" за cardinal\n",
        "\n",
        "3) ('Лада', 'B-ORG'),\n",
        "  ('Калина', 'I-ORG'),\n",
        "  ('SPORT', 'I-ORG')\n",
        "\n",
        "4) ('Авокрепеж', 'B-GPE') --- про магазин\n",
        "\n",
        "5) ('402', 'B-PRODUCT')\n",
        "\n",
        "6) ('KIA', 'B-ORG'),\n",
        "  ('Cerato', 'I-ORG'),\n",
        "  ('(', 'I-ORG'),\n",
        "  ('FORTE', 'I-ORG'),\n",
        "  ('KOUP', 'I-ORG')\n",
        "\n",
        "7) ('Продам', 'B-PERSON')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP67IAZfDCRr"
      },
      "source": [
        "Правильно размеченные примеры, которые не были размечены в данных про авто.\n",
        "\n",
        "1) ('Один', 'B-CARDINAL')\n",
        "\n",
        "2) ('12', 'B-DATE'),\n",
        "  ('.', 'I-DATE'),\n",
        "  ('12', 'I-DATE'),\n",
        "  ('.', 'I-DATE'),\n",
        "  ('2010г', 'I-DATE')\n",
        "\n",
        "3) ('69', 'B-QUANTITY'),\n",
        "  ('т', 'I-QUANTITY'),\n",
        "\n",
        "4) ('Карла', 'B-FAC'),\n",
        "  ('Маркса', 'I-FAC'),\n",
        "  ('412Б', 'I-FAC') --- про адрес"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1KG9zAhBwxM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}