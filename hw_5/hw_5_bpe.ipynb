{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_5_bpe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zd3QnkI2m15",
        "outputId": "898fb6c4-b97f-4ee3-ae54-c14e08998ba1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive') #, force_remount=True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbFHR4WI4mIm"
      },
      "source": [
        "import os\r\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aix7h4OLivFn"
      },
      "source": [
        "!pip install pymorphy2\r\n",
        "!pip install pymystem3==0.1.10\r\n",
        "!pip install rusenttokenize\r\n",
        "!pip install razdel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlWa9ZlF98B",
        "outputId": "f099f044-fdab-479c-b66e-dbee4eaab73e"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt1yTMtbGBNr",
        "outputId": "5932e7f8-6e01-41c6-919b-e1a6a36da507"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naR5i6j36FEG"
      },
      "source": [
        "**Задание 1.**\r\n",
        "\r\n",
        "Реализуйте упрощенную версию byte pair encoding (без предварительного разбивания на слова, которое есть в посте о BPE на медиуме - https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10). \r\n",
        "\r\n",
        "Алгоритм должен работать так:\r\n",
        "строки с текстом разбиваются на отдельные символы и далее в цикле из N итераций: а) считаются статистики встречаемости по парам символов и б) топ-K частотных пар склеиваются в один символ\r\n",
        "\r\n",
        "Попробуйте так токенизировать текст с разными параметрами N и K. Проанализируйте словарь уникальных слов для нескольких наборов параметров - сколько уникальных слов получилось, какой токен самый длинный?\r\n",
        "\r\n",
        "(5 баллов)\r\n",
        "\r\n",
        "Чтобы получить 1 бонусный балл - зафиксируйте получившийся словарь и токенизируйте с помощью него текст, который ранее не встречался в корпусе (возьмите рандомную новость из яндекс новостей например). Проанализируйте насколько хорошо токенизировался текст."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prcuCv6T5ReT"
      },
      "source": [
        "news = open('lenta.txt').read()[:200000]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uJ3ABX1zH4tn",
        "outputId": "e04d8421-e24c-4e3b-b76a-2fa9d4223a4d"
      },
      "source": [
        "news[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Бои у Сопоцкина и Друскеник закончились отступлением германцев. Неприятель, приблизившись с севера к'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQyCrUUUHTRf"
      },
      "source": [
        "from string import punctuation\r\n",
        "from razdel import sentenize\r\n",
        "from razdel import tokenize as razdel_tokenize\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def normalize(text):\r\n",
        "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\r\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\r\n",
        "    return ' '.join(normalized_text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiXQAMcDHtay"
      },
      "source": [
        "news_norm = normalize(news)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RvTupW6JH1mK",
        "outputId": "9dc792ce-52d6-48f6-eff0-fff99ef38a42"
      },
      "source": [
        "news_norm[:100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'бои у сопоцкина и друскеник закончились отступлением германцев неприятель приблизившись с севера к о'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFh_mnnq54RL"
      },
      "source": [
        "chars = list(news_norm)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO41u72x8SUQ",
        "outputId": "4732107b-9f57-4286-99dd-27a08cb1ff51"
      },
      "source": [
        "print(chars[:5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['б', 'о', 'и', ' ', 'у']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbCs_L8z8Tea"
      },
      "source": [
        "in_vocab = {}\r\n",
        "for i in chars:\r\n",
        "  if i in in_vocab.keys():\r\n",
        "    in_vocab[i] += 1\r\n",
        "  else:\r\n",
        "    in_vocab[i] = 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NJfTPFh8ue7",
        "outputId": "b241baa4-6890-4cca-a465-80e5ed715b77"
      },
      "source": [
        "len(in_vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArHfpvNr8wGy",
        "outputId": "d025c668-eeb3-4590-b35d-b2f588863686"
      },
      "source": [
        "for i in sorted(in_vocab, key = lambda x: in_vocab[x], reverse=True)[:10]:\r\n",
        "  print(i, in_vocab[i])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  25514\n",
            "о 17415\n",
            "е 13624\n",
            "и 13200\n",
            "а 12855\n",
            "н 10955\n",
            "т 9841\n",
            "с 9802\n",
            "р 8887\n",
            "в 8013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWWUJ4NsIJ5L"
      },
      "source": [
        "import copy\r\n",
        "def vocab_update(text, n, k):\r\n",
        "  vocab = copy.deepcopy(in_vocab)\r\n",
        "  cands = {}\r\n",
        "  for it in range(n):\r\n",
        "    for i in vocab.keys():\r\n",
        "      if i != ' ':\r\n",
        "        for j in vocab.keys():\r\n",
        "          if j != ' ':\r\n",
        "            pair = i+j\r\n",
        "            if pair in cands.keys():\r\n",
        "              continue\r\n",
        "            else:\r\n",
        "              cands[pair] = text.count(pair)\r\n",
        "    winners = sorted(cands, key = lambda x: cands[x], reverse=True)\r\n",
        "    cnt = 0\r\n",
        "    for w in winners:\r\n",
        "      if w in vocab.keys():\r\n",
        "        continue\r\n",
        "      else:\r\n",
        "        vocab[w] = cands[w]\r\n",
        "        cnt += 1\r\n",
        "      if cnt == k:\r\n",
        "        break\r\n",
        "  return vocab"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiuSd90uMkQe"
      },
      "source": [
        "test_vocab = vocab_update(news_norm, 30, 5)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD5WSJXS0sEi",
        "outputId": "19750d89-6273-4d31-8598-732cccd029a1"
      },
      "source": [
        "len(test_vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAZ71WNihV4o"
      },
      "source": [
        "def bpe_tokenizer(text, vocab):\r\n",
        "  #text_norm = normalize(text)\r\n",
        "  old_text_tok = list(text)\r\n",
        "  new_text_tok = []\r\n",
        "  while new_text_tok != old_text_tok:\r\n",
        "    if new_text_tok != []:\r\n",
        "      old_text_tok = new_text_tok\r\n",
        "    i = 0\r\n",
        "    new_text_tok = []\r\n",
        "    while i <= (len(old_text_tok)-2):\r\n",
        "      bi = old_text_tok[i] + old_text_tok[i+1]\r\n",
        "      if bi in vocab:\r\n",
        "        new_text_tok.append(bi)\r\n",
        "        i += 2\r\n",
        "      else:\r\n",
        "        new_text_tok.append(old_text_tok[i])\r\n",
        "        i += 1\r\n",
        "      if i == len(old_text_tok)-1:\r\n",
        "        new_text_tok.append(old_text_tok[i])\r\n",
        "\r\n",
        "  return new_text_tok"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKs3-vZPh_hl"
      },
      "source": [
        "tt = 'Стена была высокая. Рама тоже. А ров был глубоким.'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ARsu9j0iJdI",
        "outputId": "9bd57351-4903-4cfe-a9a5-e4d8a0e21bdb"
      },
      "source": [
        "print(bpe_tokenizer(normalize(tt), test_vocab))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ст', 'ен', 'а', ' ', 'б', 'ы', 'ла', ' ', 'вы', 'со', 'ка', 'я', ' ', 'ра', 'ма', ' ', 'то', 'же', ' ', 'а', ' ', 'ров', ' ', 'б', 'ы', 'л', ' ', 'г', 'л', 'у', 'бо', 'ки', 'м']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXArsFvFODbN"
      },
      "source": [
        "Создадим несколько вариантов словарей и посмотрим, сколько уникальных слов и какой токен самый длинный у каждой комбинации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iz5-D8Z5rRR"
      },
      "source": [
        "vocab_lens = []\r\n",
        "longest_token = []\r\n",
        "for i in [5,10,30,50]: #число итераций\r\n",
        "  for j in [3,5,10,20]: #число топовых кандидатов, которых мы добавляем в словарь на каждой итерациии\r\n",
        "    v = vocab_update(news_norm, i, j)\r\n",
        "    vocab_lens.append(len(v))\r\n",
        "    longest = max(v.keys(), key=len)\r\n",
        "    longest_token.append([longest, v[longest]])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzifark09xwq",
        "outputId": "c748a129-4e49-478d-fc9d-394562e1c5bb"
      },
      "source": [
        "k = 0\r\n",
        "for i in [5,10,30,50]:\r\n",
        "  for j in [3,5,10,20]:\r\n",
        "    print(i,j,vocab_lens[k], longest_token[k])\r\n",
        "    k+=1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 3 94 ['ст', 2639]\n",
            "5 5 104 ['ст', 2639]\n",
            "5 10 129 ['ст', 2639]\n",
            "5 20 179 ['ени', 775]\n",
            "10 3 109 ['ст', 2639]\n",
            "10 5 129 ['ст', 2639]\n",
            "10 10 179 ['ени', 775]\n",
            "10 20 279 ['ения', 301]\n",
            "30 3 169 ['ени', 775]\n",
            "30 5 229 ['ени', 775]\n",
            "30 10 379 ['ения', 301]\n",
            "30 20 679 ['сегодня', 100]\n",
            "50 3 229 ['ени', 775]\n",
            "50 5 329 ['ения', 301]\n",
            "50 10 579 ['росси', 177]\n",
            "50 20 1079 ['российск', 72]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46l82xQOOw2v"
      },
      "source": [
        "Видно, чем больше итераций и чем больше мы добавляем кандидатов из топа, тем больше у нас получается словарь и тем длинее в нем появляются токены. Также интересно отметить, что для русских новостей частотным кусочком получилась часть прилагательного \"российский\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so4mz4_XkRN0"
      },
      "source": [
        "def params_compare(text, n, k):\r\n",
        "  text_norm = normalize(text)\r\n",
        "  vocab = vocab_update(news_norm, n, k)\r\n",
        "  text_tok = bpe_tokenizer(text_norm, vocab)\r\n",
        "  return text_tok"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg0_doJL1sj2"
      },
      "source": [
        "tt1 = params_compare(tt, 5, 3)\r\n",
        "tt2 = params_compare(tt, 5, 5)\r\n",
        "tt3 = params_compare(tt, 10, 3)\r\n",
        "tt4 = params_compare(tt, 10, 5)\r\n",
        "tt5 = params_compare(tt, 30, 5)\r\n",
        "tt6 = params_compare(tt, 100, 20)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waRS_ginWcXB",
        "outputId": "c781d123-4558-4fc6-977a-068cee6b3862"
      },
      "source": [
        "for i in [tt1,tt2,tt3,tt4,tt5,tt6]:\r\n",
        "  print(i)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ст', 'ен', 'а', ' ', 'б', 'ы', 'л', 'а', ' ', 'в', 'ы', 'с', 'о', 'к', 'а', 'я', ' ', 'ра', 'м', 'а', ' ', 'то', 'ж', 'е', ' ', 'а', ' ', 'ро', 'в', ' ', 'б', 'ы', 'л', ' ', 'г', 'л', 'у', 'б', 'о', 'к', 'и', 'м']\n",
            "['ст', 'ен', 'а', ' ', 'б', 'ы', 'л', 'а', ' ', 'в', 'ы', 'с', 'о', 'к', 'а', 'я', ' ', 'ра', 'м', 'а', ' ', 'то', 'ж', 'е', ' ', 'а', ' ', 'ро', 'в', ' ', 'б', 'ы', 'л', ' ', 'г', 'л', 'у', 'б', 'о', 'к', 'и', 'м']\n",
            "['ст', 'ен', 'а', ' ', 'б', 'ы', 'л', 'а', ' ', 'в', 'ы', 'с', 'о', 'к', 'а', 'я', ' ', 'ра', 'м', 'а', ' ', 'то', 'ж', 'е', ' ', 'а', ' ', 'ро', 'в', ' ', 'б', 'ы', 'л', ' ', 'г', 'л', 'у', 'б', 'о', 'к', 'и', 'м']\n",
            "['ст', 'ен', 'а', ' ', 'б', 'ы', 'л', 'а', ' ', 'в', 'ы', 'со', 'ка', 'я', ' ', 'ра', 'м', 'а', ' ', 'то', 'ж', 'е', ' ', 'а', ' ', 'ро', 'в', ' ', 'б', 'ы', 'л', ' ', 'г', 'л', 'у', 'б', 'о', 'к', 'и', 'м']\n",
            "['ст', 'ен', 'а', ' ', 'б', 'ы', 'ла', ' ', 'вы', 'со', 'ка', 'я', ' ', 'ра', 'ма', ' ', 'то', 'же', ' ', 'а', ' ', 'ров', ' ', 'б', 'ы', 'л', ' ', 'г', 'л', 'у', 'бо', 'ки', 'м']\n",
            "['ст', 'ена', ' ', 'бы', 'ла', ' ', 'вы', 'со', 'кая', ' ', 'ра', 'ма', ' ', 'то', 'же', ' ', 'а', ' ', 'ров', ' ', 'был', ' ', 'гл', 'уб', 'ок', 'им']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYuFrrWxQHHs"
      },
      "source": [
        "Токенизируем часть наших текстов с разными параметрами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aprtv9KE2Rin",
        "outputId": "da1299c5-d425-4075-efe7-a6d465df5358"
      },
      "source": [
        "for i in [5,10,30]:\r\n",
        "  for j in [3,5,10,20]:\r\n",
        "    print(f'N = {{}}, K = {{}}, {{}}'.format(i, j, params_compare(news[:100], i, j)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N = 5, K = 3, ['б', 'о', 'и', ' ', 'у', ' ', 'с', 'о', 'по', 'ц', 'к', 'и', 'на', ' ', 'и', ' ', 'д', 'р', 'у', 'с', 'к', 'ен', 'и', 'к', ' ', 'з', 'а', 'ко', 'н', 'ч', 'и', 'л', 'и', 'с', 'ь', ' ', 'о', 'т', 'ст', 'у', 'п', 'л', 'ен', 'и', 'е', 'м', ' ', 'г', 'е', 'р', 'м', 'ан', 'ц', 'е', 'в', ' ', 'н', 'е', 'пр', 'и', 'я', 'т', 'е', 'л', 'ь', ' ', 'пр', 'и', 'б', 'л', 'и', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'в', 'е', 'ра', ' ', 'к']\n",
            "N = 5, K = 5, ['б', 'о', 'и', ' ', 'у', ' ', 'с', 'о', 'по', 'ц', 'к', 'и', 'на', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'и', 'к', ' ', 'з', 'а', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'о', 'т', 'ст', 'у', 'п', 'ле', 'ни', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'л', 'ь', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'в', 'ер', 'а', ' ', 'к']\n",
            "N = 5, K = 10, ['б', 'о', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'к', 'ин', 'а', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'и', 'к', ' ', 'за', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'ве', 'ра', ' ', 'к']\n",
            "N = 5, K = 20, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 10, K = 3, ['б', 'о', 'и', ' ', 'у', ' ', 'с', 'о', 'по', 'ц', 'к', 'и', 'на', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'и', 'к', ' ', 'з', 'а', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'о', 'т', 'ст', 'у', 'п', 'ле', 'ни', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'л', 'ь', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'в', 'ер', 'а', ' ', 'к']\n",
            "N = 10, K = 5, ['б', 'о', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'к', 'ин', 'а', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'и', 'к', ' ', 'за', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'ве', 'ра', ' ', 'к']\n",
            "N = 10, K = 10, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 10, K = 20, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'чи', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'ц', 'ев', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'при', 'бл', 'из', 'ив', 'ши', 'с', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 30, K = 3, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 30, K = 5, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'ц', 'ев', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'при', 'б', 'ли', 'з', 'ив', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 30, K = 10, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'кон', 'чи', 'ли', 'сь', ' ', 'от', 'ст', 'уп', 'лени', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'при', 'бл', 'из', 'ив', 'ши', 'сь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 30, K = 20, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'др', 'ус', 'ке', 'ник', ' ', 'за', 'кон', 'чи', 'ли', 'сь', ' ', 'от', 'ст', 'уп', 'лени', 'ем', ' ', 'ге', 'рм', 'ан', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'пр', 'иб', 'ли', 'зи', 'вш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUlWDj-qXC7E",
        "outputId": "22a6da66-02c6-4735-932f-3f2851705ae2"
      },
      "source": [
        "for i in [[50,10],[100,10]]:\r\n",
        "  print(f'N = {{}}, K = {{}}, {{}}'.format(i[0], i[1], params_compare(news[:100], i[0], i[1])))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N = 50, K = 10, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'др', 'ус', 'ке', 'ник', ' ', 'за', 'кон', 'чи', 'ли', 'сь', ' ', 'от', 'ст', 'уп', 'лени', 'ем', ' ', 'ге', 'рм', 'ан', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'пр', 'иб', 'ли', 'зи', 'вш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n",
            "N = 100, K = 10, ['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'др', 'ус', 'ке', 'ник', ' ', 'за', 'кон', 'чи', 'лись', ' ', 'от', 'ступ', 'лени', 'ем', ' ', 'ге', 'рм', 'ан', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'пр', 'иб', 'ли', 'зи', 'вш', 'ись', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toUGK_HvQDf6"
      },
      "source": [
        "При повышении значений параметров выделяются все более крупные кусочки слов, что в принципе является ожидаемым следствием"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95K6-LEvRECg"
      },
      "source": [
        "Попробуем разметить какой-нибудь новый текст"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0dpA1WHb3UA",
        "outputId": "e8cd6992-4458-4a63-b3fb-30bd4943967f"
      },
      "source": [
        "text_norm = normalize(tn)\r\n",
        "text_tok = bpe_tokenizer(text_norm, test_vocab) #N = 30 K = 5\r\n",
        "print(text_tok)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['но', 'вы', 'й', ' ', 'же', 'ле', 'з', 'но', 'до', 'ро', 'ж', 'ны', 'й', ' ', 'про', 'ек', 'т', ' ', 'ра', 'з', 'у', 'ме', 'ет', 'ся', ' ', 'не', ' ', 'ро', 'ди', 'л', 'ся', ' ', 'из', ' ', 'ни', 'от', 'к', 'уд', 'а', ' ', 'т', 'у', 'р', 'ци', 'я', ' ', 'ст', 'ал', 'а', ' ', 'ин', 'те', 'ре', 'со', 'ва', 'ть', ' ', 'ки', 'та', 'й', 'ск', 'их', ' ', 'в', 'не', 'ш', 'не', 'по', 'ли', 'ти', 'че', 'ск', 'их', ' ', 'ст', 'ра', 'те', 'го', 'в', ' ', 'е', 'щ', 'е', ' ', 'с', ' ', 'x', 'i', 'x', ' ', 'ве', 'ка', ' ', 'та', 'к', ' ', 'ф', 'ил', 'ос', 'о', 'ф', ' ', 'та', 'н', 'ь', ' ', 'с', 'ы', 'т', 'у', 'н', ' ', 'в', ' ', 'с', 'во', 'ей', ' ', 'ра', 'бо', 'те', ' ', '«', ' ', 'у', 'че', 'ние', ' ', 'о', ' ', 'г', 'у', 'ма', 'нн', 'ос', 'ти', ' ', '»', ' ', 'пр', 'из', 'ы', 'ва', 'л', ' ', 'со', 'з', 'да', 'ть', ' ', 'па', 'на', 'з', 'и', 'ат', 'ск', 'ий', ' ', 'со', 'ю', 'з', ' ', 'от', ' ', 'т', 'у', 'р', 'ци', 'и', ' ', 'до', ' ', 'ки', 'та', 'я', ' ', 'а', ' ', 'во', ' ', 'в', 'то', 'ро', 'й', ' ', 'по', 'ло', 'ви', 'не', ' ', 'х', 'х', ' ', 'ве', 'ка', ' ', 'в', ' ', 'те', 'ор', 'ии', ' ', '«', ' ', 'тр', 'е', 'х', ' ', 'ми', 'ров', ' ', '»', ' ', 'ма', 'о', ' ', 'ц', 'з', 'э', 'д', 'у', 'на', ' ', 'т', 'у', 'р', 'ци', 'и', ' ', 'от', 'во', 'ди', 'ло', 'с', 'ь', ' ', 'ва', 'ж', 'но', 'е', ' ', 'ме', 'сто', ' ', 'в', ' ', 'со', 'з', 'да', 'ни', 'и', ' ', 'по', 'я', 'са', ' ', 'б', 'ез', 'оп', 'ас', 'но', 'сти', ' ', 'пр', 'от', 'ив', ' ', 'сс', 'с', 'р', ' ', 'г', 'ла', 'в', 'но', 'й', ' ', 'по', ' ', 'его', ' ', 'м', 'не', 'ни', 'ю', ' ', 'г', 'ег', 'ем', 'он', 'ис', 'тс', 'ко', 'й', ' ', 'си', 'л', 'ы', ' ', 'в', ' ', 'ми', 'ре', ' ', 'от', 'ме', 'ча', 'ет', ' ', 'ки', 'та', 'ис', 'т', ' ', 'сп', 'е', 'ци', 'ал', 'ис', 'т', ' ', 'в', ' ', 'об', 'ла', 'сти', ' ', 'ме', 'ж', 'д', 'у', 'на', 'ро', 'дн', 'ых', ' ', 'от', 'но', 'ше', 'ни', 'й', ' ', 'ол', 'ег', ' ', 'ти', 'мо', 'ф', 'е', 'ев']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_6M9NNxXo18",
        "outputId": "c80f547d-0599-4ec5-c01c-70b069ee8e74"
      },
      "source": [
        "tn = 'Новый железнодорожный проект, разумеется, не родился из ниоткуда. Турция стала интересовать китайских внешнеполитических стратегов еще с XIX века. Так, философ Тань Сытун в своей работе «Учение о гуманности» призывал создать паназиатский союз от Турции до Китая. А во второй половине ХХ века в теории «трех миров» Мао Цзэдуна Турции отводилось важное место в создании пояса безопасности против СССР, главной, по его мнению, гегемонистской силы в мире, отмечает китаист, специалист в области международных отношений Олег Тимофеев.'\r\n",
        "print(params_compare(tn, 30, 10))\r\n",
        "print(params_compare(tn, 50, 20))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['но', 'вы', 'й', ' ', 'же', 'ле', 'з', 'но', 'до', 'ро', 'ж', 'ны', 'й', ' ', 'пр', 'ое', 'кт', ' ', 'раз', 'у', 'ме', 'ет', 'ся', ' ', 'не', ' ', 'ро', 'ди', 'л', 'ся', ' ', 'из', ' ', 'ни', 'от', 'ку', 'да', ' ', 'ту', 'р', 'ци', 'я', ' ', 'ст', 'ал', 'а', ' ', 'ин', 'те', 'ре', 'со', 'ва', 'ть', ' ', 'ки', 'та', 'йс', 'ки', 'х', ' ', 'вн', 'е', 'ш', 'не', 'по', 'ли', 'ти', 'че', 'ск', 'их', ' ', 'ст', 'ра', 'те', 'го', 'в', ' ', 'е', 'ще', ' ', 'с', ' ', 'x', 'i', 'x', ' ', 'ве', 'ка', ' ', 'та', 'к', ' ', 'ф', 'ил', 'ос', 'о', 'ф', ' ', 'тан', 'ь', ' ', 'с', 'ы', 'ту', 'н', ' ', 'в', ' ', 'св', 'ое', 'й', ' ', 'ра', 'бо', 'те', ' ', '«', ' ', 'у', 'че', 'ние', ' ', 'о', ' ', 'г', 'у', 'ма', 'нн', 'ости', ' ', '»', ' ', 'пр', 'из', 'ыв', 'ал', ' ', 'со', 'з', 'да', 'ть', ' ', 'па', 'на', 'зи', 'ат', 'ск', 'ий', ' ', 'со', 'ю', 'з', ' ', 'от', ' ', 'ту', 'р', 'ции', ' ', 'до', ' ', 'ки', 'та', 'я', ' ', 'а', ' ', 'во', ' ', 'в', 'то', 'ро', 'й', ' ', 'по', 'ло', 'ви', 'не', ' ', 'х', 'х', ' ', 'ве', 'ка', ' ', 'в', ' ', 'те', 'ор', 'ии', ' ', '«', ' ', 'тре', 'х', ' ', 'ми', 'ров', ' ', '»', ' ', 'ма', 'о', ' ', 'ц', 'з', 'э', 'ду', 'на', ' ', 'ту', 'р', 'ции', ' ', 'от', 'во', 'ди', 'ло', 'сь', ' ', 'ва', 'ж', 'но', 'е', ' ', 'ме', 'сто', ' ', 'в', ' ', 'со', 'з', 'да', 'ни', 'и', ' ', 'по', 'я', 'са', ' ', 'бе', 'зо', 'па', 'с', 'ност', 'и', ' ', 'пр', 'от', 'ив', ' ', 'сс', 'с', 'р', ' ', 'г', 'ла', 'вн', 'ой', ' ', 'по', ' ', 'его', ' ', 'м', 'не', 'ни', 'ю', ' ', 'г', 'ег', 'ем', 'он', 'ис', 'тс', 'кой', ' ', 'си', 'л', 'ы', ' ', 'в', ' ', 'ми', 'ре', ' ', 'от', 'ме', 'ча', 'ет', ' ', 'ки', 'та', 'ист', ' ', 'сп', 'е', 'ци', 'ал', 'ист', ' ', 'в', ' ', 'об', 'ла', 'сти', ' ', 'ме', 'жд', 'у', 'на', 'ро', 'дн', 'ых', ' ', 'от', 'но', 'ше', 'ни', 'й', ' ', 'ол', 'ег', ' ', 'ти', 'мо', 'ф', 'е', 'ев']\n",
            "['но', 'вы', 'й', ' ', 'же', 'ле', 'зн', 'од', 'ор', 'ож', 'ный', ' ', 'пр', 'ое', 'кт', ' ', 'ра', 'зу', 'ме', 'ется', ' ', 'не', ' ', 'ро', 'ди', 'лся', ' ', 'из', ' ', 'ни', 'от', 'ку', 'да', ' ', 'ту', 'р', 'ция', ' ', 'ст', 'ала', ' ', 'инте', 'ре', 'со', 'вать', ' ', 'ки', 'та', 'йс', 'ких', ' ', 'вн', 'еш', 'не', 'по', 'ли', 'ти', 'ческ', 'их', ' ', 'стра', 'те', 'гов', ' ', 'ещ', 'е', ' ', 'с', ' ', 'x', 'i', 'x', ' ', 'ве', 'ка', ' ', 'так', ' ', 'фи', 'ло', 'со', 'ф', ' ', 'та', 'нь', ' ', 'с', 'ыт', 'ун', ' ', 'в', ' ', 'св', 'ое', 'й', ' ', 'рабо', 'те', ' ', '«', ' ', 'уч', 'ение', ' ', 'о', ' ', 'гу', 'ма', 'нн', 'ости', ' ', '»', ' ', 'пр', 'из', 'ыв', 'ал', ' ', 'со', 'зд', 'ать', ' ', 'па', 'на', 'зи', 'ат', 'ский', ' ', 'со', 'ю', 'з', ' ', 'от', ' ', 'ту', 'р', 'ции', ' ', 'до', ' ', 'ки', 'та', 'я', ' ', 'а', ' ', 'во', ' ', 'вт', 'ор', 'ой', ' ', 'по', 'ло', 'ви', 'не', ' ', 'х', 'х', ' ', 'ве', 'ка', ' ', 'в', ' ', 'те', 'ор', 'ии', ' ', '«', ' ', 'тре', 'х', ' ', 'ми', 'ров', ' ', '»', ' ', 'ма', 'о', ' ', 'ц', 'з', 'э', 'ду', 'на', ' ', 'ту', 'р', 'ции', ' ', 'от', 'во', 'ди', 'ло', 'сь', ' ', 'ва', 'жн', 'ое', ' ', 'мест', 'о', ' ', 'в', ' ', 'со', 'зд', 'ании', ' ', 'по', 'я', 'са', ' ', 'бе', 'зо', 'па', 'сн', 'ости', ' ', 'пр', 'от', 'ив', ' ', 'сс', 'ср', ' ', 'гл', 'ав', 'ной', ' ', 'по', ' ', 'его', ' ', 'мн', 'ению', ' ', 'ге', 'ге', 'мо', 'нист', 'ской', ' ', 'сил', 'ы', ' ', 'в', ' ', 'ми', 'ре', ' ', 'от', 'ме', 'ча', 'ет', ' ', 'ки', 'та', 'ист', ' ', 'сп', 'ец', 'иа', 'ли', 'ст', ' ', 'в', ' ', 'об', 'ла', 'сти', ' ', 'ме', 'жд', 'ун', 'ар', 'од', 'ных', ' ', 'от', 'но', 'шени', 'й', ' ', 'ол', 'ег', ' ', 'ти', 'мо', 'фе', 'ев']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfH3DNPHRRsF"
      },
      "source": [
        "В принципе, кажется, что результат получается довольно близким к тому, что было при тех же параметрах на тексте из корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVbCnI0Z4xSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBbmswuXMmL0"
      },
      "source": [
        "**Задание 2.**\r\n",
        "\r\n",
        "Обучите токенизатор из tokenizers на текстовом корпусе. Рассчитайте статистики для idf по корпусу, используя обученный словарь (разбейте корпус на \"документы\" по новым строкам, каждый \"документ\" токенизируйте, для каждого слова посчитайте, в скольких документах оно встречается и рассчитайте idf разделив общее количество документов на это число, возьмите логарифм от полученного числа). \r\n",
        "Векторизуйте текст (в мешок слов) аналогично TfidfVectorizer, используя токенизатор и idf статистики (инициализируйте*** пустую матрицу размером (N документов, K слов в словаре) и в цикле по всем документам постепенно заполните ее - токенизируйте документ, рассчитайте TF каждого слова (количество вхождений в документе поделить на общее количество слов в документе), умножьте TF на IDF и, используя индексы слов в словаре, запишите получившееся значение в матрицу)\r\n",
        "\r\n",
        "Формулу для TFIDF можете уточнить тут -  https://ru.wikipedia.org/wiki/TF-IDF\r\n",
        "\r\n",
        "***Чтобы инициализировать разреженную матрицу используйте scipy:\r\n",
        "from scipy.sparse import lil_matrix\r\n",
        "X = lil_matrix(N, K)\r\n",
        "\r\n",
        "Обучите классификатор на полученных векторах и оцените на кросс-валидации. \r\n",
        "\r\n",
        "(5 баллов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPzr9vJ5NkaM",
        "outputId": "b34f24a1-1e7c-412f-a1d3-6790e45008d8"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1Vrcynyn4_"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report, accuracy_score\r\n",
        "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1OnxVKVNngz"
      },
      "source": [
        "from tokenizers import CharBPETokenizer, Tokenizer"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEkkb-ltB1Ca"
      },
      "source": [
        "def tf(document, terms):\r\n",
        "    words = normalize(document)\r\n",
        "    total_words = len(words)\r\n",
        "    doc_counter = Counter(words)\r\n",
        "    for word in doc_counter:\r\n",
        "      doc_counter[word] /= total_words\r\n",
        "    tfs = [0 for _ in range(len(terms))]\r\n",
        "    for i in range(len(terms)):\r\n",
        "      tfs[i] = doc_counter[terms[i]]\r\n",
        "    return tfs\r\n",
        "\r\n",
        "def count_docs_with_word(word, docs):\r\n",
        "    counter = 0\r\n",
        "    for doc in docs:\r\n",
        "        if word in doc:\r\n",
        "            counter += 1\r\n",
        "    return counter\r\n",
        "\r\n",
        "def idf(documents, terms):\r\n",
        "    idfs = [0 for _ in range(len(terms))]\r\n",
        "    total_docs = len(documents)\r\n",
        "    for i in range(len(terms)):\r\n",
        "      docs_with_word = count_docs_with_word(terms[i], documents)\r\n",
        "      idf = 1 + math.log((total_docs+1) / (docs_with_word+1))\r\n",
        "      idfs[i] = idf\r\n",
        "    return idfs\r\n",
        "\r\n",
        "def td_idf(tf, idf, terms):\r\n",
        "    return [tf[i] * idf[i] for i in range(len(terms))]\r\n",
        "\r\n",
        "def build_tfidf(corpus, document, idfs, terms):\r\n",
        "    doc_tf = tf(document, terms)\r\n",
        "    return td_idf(doc_tf, idfs, terms)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jo4jcdmiMsTz",
        "outputId": "43e86eea-c22b-463d-c8f3-30299de595cd"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv('dataset_ok.csv')[:30000]\r\n",
        "df.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>наебалово века, для долбаёбов\\n</td>\n",
              "      <td>INSULT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>вся дума в таком же положении😁\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>а в каком месте массовое столкновение? шрайбик...</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>значит ли это, что контроль за вывозом крупног...</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>вам не нужен щеночек? очень хорошие 🐶🥰\\n</td>\n",
              "      <td>NORMAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   label\n",
              "0                    наебалово века, для долбаёбов\\n  INSULT\n",
              "1                   вся дума в таком же положении😁\\n  NORMAL\n",
              "2  а в каком месте массовое столкновение? шрайбик...  NORMAL\n",
              "3  значит ли это, что контроль за вывозом крупног...  NORMAL\n",
              "4           вам не нужен щеночек? очень хорошие 🐶🥰\\n  NORMAL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uvo4h42rFdX",
        "outputId": "66e1f614-2f51-432b-c86a-22cdb74956ca"
      },
      "source": [
        "df.label.value_counts()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NORMAL       25440\n",
              "INSULT        3577\n",
              "THREAT         685\n",
              "OBSCENITY      298\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGsWltESJMvJ"
      },
      "source": [
        "#normal_ids = df.index[df['label'] == 'NORMAL'].tolist()"
      ],
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBkvcMKAJuWK"
      },
      "source": [
        "#df = df.drop(normal_ids[:51055]).reset_index(drop=True)"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N9P0GkLKjHk",
        "outputId": "d61910a9-2544-4421-eb90-1458909249c1"
      },
      "source": [
        "#df.label.value_counts()"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NORMAL       10000\n",
              "INSULT        8584\n",
              "THREAT        1666\n",
              "OBSCENITY      682\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR6KdfTaYQAO"
      },
      "source": [
        "df['text'].to_csv('corpus.txt', index=None)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lzHBMBMNNxM"
      },
      "source": [
        "tok_sub = CharBPETokenizer()\r\n",
        "tok_sub.train('corpus.txt', vocab_size=3000, min_frequency=10,)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHQ92dNuYXZ4",
        "outputId": "3754f6f8-036f-4785-953a-00ea1653eafd"
      },
      "source": [
        "print(tok_sub.encode(df.loc[0, 'text']).tokens)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['на', 'еба', 'ло', 'во</w>', 'ве', 'ка</w>', ',</w>', 'для</w>', 'долба', 'ё', 'бов</w>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI-H23dGYe33",
        "outputId": "f14fe8d3-4fd0-45f6-ace0-8149c2337ea2"
      },
      "source": [
        "len(tok_sub.get_vocab())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiOWxPbsZL2I"
      },
      "source": [
        "vocab = list(tok_sub.get_vocab().keys())"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZd6xtv5ZQXR",
        "outputId": "a622a049-07d3-4116-fcd1-0715eaf6be75"
      },
      "source": [
        "vocab[:5]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['везде</w>', 'счаст', 'ср</w>', '3</w>', '🌆']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FeAL2tZZADT"
      },
      "source": [
        "from collections import Counter\r\n",
        "import operator\r\n",
        "import math"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W31AE9-1ZhnA"
      },
      "source": [
        "texts = df['text']"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6-Jc-b3ExdX"
      },
      "source": [
        "all_idfs = idf(texts, vocab)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAAMyPbBEzRQ",
        "outputId": "8233d7fc-ec3f-4965-b121-c034c6d76885"
      },
      "source": [
        "len(vocab), len(all_idfs)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 3000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5abBdtLEv8j"
      },
      "source": [
        "tf_idf_total = []\r\n",
        "for document in texts:\r\n",
        "  tf_idf_total.append(build_tfidf(texts, document, all_idfs, vocab))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "deJWnQ5A7pla",
        "outputId": "99fabde3-1f79-4bf0-8c7c-df51ee7b82a7"
      },
      "source": [
        "import pandas as pd\r\n",
        "frame = pd.DataFrame(tf_idf_total, columns=vocab, index=texts)\r\n",
        "frame.head()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>везде&lt;/w&gt;</th>\n",
              "      <th>счаст</th>\n",
              "      <th>ср&lt;/w&gt;</th>\n",
              "      <th>3&lt;/w&gt;</th>\n",
              "      <th>🌆</th>\n",
              "      <th>pro</th>\n",
              "      <th>🐑</th>\n",
              "      <th>ез</th>\n",
              "      <th>в&lt;/w&gt;</th>\n",
              "      <th>ху</th>\n",
              "      <th>дра</th>\n",
              "      <th>🐾</th>\n",
              "      <th>людя</th>\n",
              "      <th>🦷&lt;/w&gt;</th>\n",
              "      <th>вы</th>\n",
              "      <th>бра</th>\n",
              "      <th>понятно&lt;/w&gt;</th>\n",
              "      <th>све</th>\n",
              "      <th>😠</th>\n",
              "      <th>ч&lt;/w&gt;</th>\n",
              "      <th>не</th>\n",
              "      <th>🙆</th>\n",
              "      <th>ε&lt;/w&gt;</th>\n",
              "      <th>живо</th>\n",
              "      <th>украи</th>\n",
              "      <th>то&lt;/w&gt;</th>\n",
              "      <th>власти&lt;/w&gt;</th>\n",
              "      <th>писа</th>\n",
              "      <th>посмот</th>\n",
              "      <th>понрави</th>\n",
              "      <th>прекрас</th>\n",
              "      <th>💐&lt;/w&gt;</th>\n",
              "      <th>него&lt;/w&gt;</th>\n",
              "      <th>🏆</th>\n",
              "      <th>🦺</th>\n",
              "      <th>всю&lt;/w&gt;</th>\n",
              "      <th>пишите&lt;/w&gt;</th>\n",
              "      <th>банк&lt;/w&gt;</th>\n",
              "      <th>стой</th>\n",
              "      <th>💖&lt;/w&gt;</th>\n",
              "      <th>...</th>\n",
              "      <th>взять&lt;/w&gt;</th>\n",
              "      <th>отнош</th>\n",
              "      <th>чес</th>\n",
              "      <th>✋&lt;/w&gt;</th>\n",
              "      <th>⬆&lt;/w&gt;</th>\n",
              "      <th>🐓&lt;/w&gt;</th>\n",
              "      <th>ди&lt;/w&gt;</th>\n",
              "      <th>ение&lt;/w&gt;</th>\n",
              "      <th>хаба</th>\n",
              "      <th>дь&lt;/w&gt;</th>\n",
              "      <th>гру</th>\n",
              "      <th>зм&lt;/w&gt;</th>\n",
              "      <th>настро</th>\n",
              "      <th>‘&lt;/w&gt;</th>\n",
              "      <th>ль&lt;/w&gt;</th>\n",
              "      <th>россию&lt;/w&gt;</th>\n",
              "      <th>раба</th>\n",
              "      <th>убе</th>\n",
              "      <th>все&lt;/w&gt;</th>\n",
              "      <th>нул&lt;/w&gt;</th>\n",
              "      <th>нь</th>\n",
              "      <th>п&lt;/w&gt;</th>\n",
              "      <th>через&lt;/w&gt;</th>\n",
              "      <th>🤕</th>\n",
              "      <th>страну&lt;/w&gt;</th>\n",
              "      <th>теля</th>\n",
              "      <th>🙄</th>\n",
              "      <th>чере</th>\n",
              "      <th>рам&lt;/w&gt;</th>\n",
              "      <th>ние&lt;/w&gt;</th>\n",
              "      <th>вания&lt;/w&gt;</th>\n",
              "      <th>🐖</th>\n",
              "      <th>🤠</th>\n",
              "      <th>бы</th>\n",
              "      <th>➕</th>\n",
              "      <th>на</th>\n",
              "      <th>этом&lt;/w&gt;</th>\n",
              "      <th>⚫</th>\n",
              "      <th>😵</th>\n",
              "      <th>мы</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>наебалово века, для долбаёбов\\n</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>вся дума в таком же положении😁\\n</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>а в каком месте массовое столкновение? шрайбикус решил набрать лайков? пиши нормальный комментарий, а не эту хрень\\n</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>значит ли это, что контроль за вывозом крупногабаритного мусора и работой регионального оператора осуществляют жители домов?\\n</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>вам не нужен щеночек? очень хорошие 🐶🥰\\n</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    везде</w>  счаст  ...    😵   мы\n",
              "text                                                                  ...          \n",
              "наебалово века, для долбаёбов\\n                           0.0    0.0  ...  0.0  0.0\n",
              "вся дума в таком же положении😁\\n                          0.0    0.0  ...  0.0  0.0\n",
              "а в каком месте массовое столкновение? шрайбику...        0.0    0.0  ...  0.0  0.0\n",
              "значит ли это, что контроль за вывозом крупнога...        0.0    0.0  ...  0.0  0.0\n",
              "вам не нужен щеночек? очень хорошие 🐶🥰\\n                  0.0    0.0  ...  0.0  0.0\n",
              "\n",
              "[5 rows x 3000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDsJ-4nyc3n"
      },
      "source": [
        "clf = SGDClassifier(loss=\"log\", max_iter=50)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KwcX3Mn03NH"
      },
      "source": [
        "clf_2 = MultinomialNB(alpha=0.5, fit_prior='bool', class_prior=[1,1,1,1])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj0bpr7lO7U_"
      },
      "source": [
        "clf_3 = SGDClassifier(loss=\"log\", max_iter=50, alpha=0.0001, class_weight='balanced')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFhrY_x8zEgu"
      },
      "source": [
        "X = tf_idf_total"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phy1utM0zGVW",
        "outputId": "3cb89531-79f9-4515-b450-ec4619c2ac9a"
      },
      "source": [
        "len(X[0])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_k6T0GWy8JA"
      },
      "source": [
        "y = np.array(df.label)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SDGneSAyywi",
        "outputId": "76a36007-ede8-4ff2-85c2-71f4a83e4925"
      },
      "source": [
        "cross_val_score(clf, X, y, scoring=\"f1_macro\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.23076987, 0.23417476, 0.23567437, 0.23515591, 0.23281145])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpWvazfSzOxZ",
        "outputId": "4facfd8d-cd19-4f6b-e52b-6eaa4f6fade5"
      },
      "source": [
        "cross_val_score(clf_2, X, y, scoring=\"f1_macro\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.23780051, 0.24171987, 0.23271911, 0.23563234, 0.23939572])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy5u434bUhlD",
        "outputId": "4f8f6038-b895-483e-ad6a-82a60585e7e4"
      },
      "source": [
        "cross_val_score(clf_3, X, y, scoring=\"f1_macro\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.30919936, 0.33169457, 0.355256  , 0.32147213, 0.30374472])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Pywgp63XEk",
        "outputId": "5e54162f-7e12-47ed-f1ca-885a1a78a9c9"
      },
      "source": [
        "cross_val_score(clf, X, y, scoring=\"f1_micro\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.847     , 0.84816667, 0.84816667, 0.84916667, 0.84783333])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpno0O838UF-",
        "outputId": "48aba0fb-5b32-4c7c-a3af-2bae9a501c4f"
      },
      "source": [
        "cross_val_score(clf_2, X, y, scoring=\"f1_micro\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.84266667, 0.84166667, 0.84266667, 0.84116667, 0.8405    ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1TykANlPHfe",
        "outputId": "6ee82994-be8c-4acf-bbe7-c0dea731370a"
      },
      "source": [
        "cross_val_score(clf_3, X, y, scoring=\"f1_micro\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81566667, 0.83483333, 0.84133333, 0.82783333, 0.83      ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnjGgwFb8r1Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}