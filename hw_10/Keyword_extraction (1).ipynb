{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Keyword_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w-M0idzEir7C"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqOD8mrsir2F"
      },
      "source": [
        "## Извлечение ключевых слов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS-WNHq5qSht",
        "outputId": "cf0193fa-7b7d-4f59-fadf-1806dab65e1a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #, force_remount=True"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arZYIaU2qmQr"
      },
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR-7RG9y3LgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560fef8a-9342-41a2-9dda-d3f2565442b8"
      },
      "source": [
        "!pip install pymorphy2\n",
        "!pip install pymystem3==0.1.10\n",
        "!pip install rusenttokenize\n",
        "!pip install razdel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 11.1MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Collecting pymystem3==0.1.10\n",
            "  Downloading https://files.pythonhosted.org/packages/51/56/57e550b53587719e92330a79c7c0f555402d953b00700efae6d5ca53cdef/pymystem3-0.1.10-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3==0.1.10) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (3.0.4)\n",
            "Installing collected packages: pymystem3\n",
            "  Found existing installation: pymystem3 0.2.0\n",
            "    Uninstalling pymystem3-0.2.0:\n",
            "      Successfully uninstalled pymystem3-0.2.0\n",
            "Successfully installed pymystem3-0.1.10\n",
            "Collecting rusenttokenize\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Installing collected packages: rusenttokenize\n",
            "Successfully installed rusenttokenize-0.0.5\n",
            "Collecting razdel\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlNbPdWms6iw",
        "outputId": "0bc1a83c-5c4b-4065-d641-848b1e8590c0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCtJjk0Pir21"
      },
      "source": [
        "import json, os\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy2.tokenizers import simple_word_tokenize\n",
        "import numpy as np\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "morph = MorphAnalyzer()\n",
        "stops = set(stopwords.words('russian'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncmRYt1Wir26"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPWoCJMrir3C"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_wL_Shrir3D"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QdHB2tPir3F"
      },
      "source": [
        "Возьмем данные вот отсюда - https://github.com/mannefedov/ru_kw_eval_datasets Там лежат 4 датасета (статьи с хабра, с Russia Today, Независимой газеты и научные статьи с Киберленинки). Датасет НГ самый маленький, поэтому возьмем его в качестве примера."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x447mnfz3U92",
        "outputId": "d9bbb92e-36ce-41d1-fa6a-8ae7cddd343f"
      },
      "source": [
        "!wget https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_0.jsonlines.zip\n",
        "!wget https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_1.jsonlines.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-25 10:43:27--  https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_0.jsonlines.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_0.jsonlines.zip [following]\n",
            "--2021-06-25 10:43:27--  https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_0.jsonlines.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2987145 (2.8M) [application/zip]\n",
            "Saving to: ‘ng_0.jsonlines.zip.2’\n",
            "\n",
            "ng_0.jsonlines.zip. 100%[===================>]   2.85M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-06-25 10:43:28 (24.0 MB/s) - ‘ng_0.jsonlines.zip.2’ saved [2987145/2987145]\n",
            "\n",
            "--2021-06-25 10:43:28--  https://github.com/mannefedov/ru_kw_eval_datasets/raw/master/data/ng_1.jsonlines.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_1.jsonlines.zip [following]\n",
            "--2021-06-25 10:43:28--  https://raw.githubusercontent.com/mannefedov/ru_kw_eval_datasets/master/data/ng_1.jsonlines.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3074290 (2.9M) [application/zip]\n",
            "Saving to: ‘ng_1.jsonlines.zip.2’\n",
            "\n",
            "ng_1.jsonlines.zip. 100%[===================>]   2.93M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-06-25 10:43:29 (34.6 MB/s) - ‘ng_1.jsonlines.zip.2’ saved [3074290/3074290]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgG_GpjGir3J",
        "outputId": "72fe1dc7-03e8-4a4d-9619-29af948e0b14"
      },
      "source": [
        "!unzip ng_0.jsonlines.zip \n",
        "!unzip ng_1.jsonlines.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ng_0.jsonlines.zip\n",
            "replace ng_0.jsonlines? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ng_0.jsonlines          \n",
            "Archive:  ng_1.jsonlines.zip\n",
            "replace ng_1.jsonlines? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ng_1.jsonlines          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZNVe-Ggir3K"
      },
      "source": [
        "PATH_TO_DATA = './'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzrnml1dir3L"
      },
      "source": [
        "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA) if file.endswith('jsonlines')]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757BCXtvir3T"
      },
      "source": [
        "Объединим файлы в один датасет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D69LdpLir3U"
      },
      "source": [
        "data = pd.concat([pd.read_json(file, lines=True) for file in files][:5], axis=0, ignore_index=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2UlxlhKir3V",
        "outputId": "0d4a880e-91ed-431a-8074-2a7b3162ecae"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1987, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "ihI3vz2uir3W",
        "outputId": "0096bd21-2566-4a47-92cc-5514fec70c08"
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keywords</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[школа, образовательные стандарты, литература, история, фгос]</td>\n",
              "      <td>Ольга Васильева обещала \"НГ\" не перегружать школьников</td>\n",
              "      <td>https://amp.ng.ru/?p=http://www.ng.ru/education/2018-03-22/8_7195_school.html</td>\n",
              "      <td>В среду состоялось отложенное заседание Совета по федеральным государственным образовательным стандартам (ФГОС) при Министерстве образования и науки РФ. Собрание должно было состояться еще в понедельник, но было перенесено по просьбе членов совета. И вот пришло сообщение, что общественники выразили согласие с позицией министерства. Новые ФГОСы приняты.\\nНа вчерашнем заседании был принят ФГОС по начальной общеобразовательной школе. До 28 марта продлятся косультации по ФГОСам для средней школы.\\nНапомним, что накануне Гильдия словесников разместила открытое письмо на имя министра образования и науки РФ Ольги Васильевой. По мнению авторов письма, новые ФГОСы грубо нарушают права детей, уже проучившихся по существующему стандарту до 6-го класса. Приняв новый стандарт, Министерство образования дает право контролирующим органам ловить детей на незнании большого списка произведений (235 за пять лет обучения). «Это исключает возможность полноценного их освоения, создает риск формального, п...</td>\n",
              "      <td>Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[красота, законы]</td>\n",
              "      <td>У красоты собственные закон и воля</td>\n",
              "      <td>https://amp.ng.ru/?p=http://www.ng.ru/style/2018-03-19/8_7192_beauty.html</td>\n",
              "      <td>Хорошо, когда красота в глазах смотрящего живет свободно или хотя бы занимает широкий угол зрения. Плохо было б, если б она вовсе не озаряла своим светом космическую темень пустоты зрачка. Слава богу, такое вряд ли возможно. \\nА случается, что красота уходит. Почему вдруг? И куда она девается, когда в один из философских обходов своего организма вы, еще недавно гордый ее обладатель, обескураженно ее  недосчитываетесь? \\nВообразите: прелестнейшее из созданий – ваша кошка пластичнейшими движениями рвет банкноту за банкнотой, забирается на карниз по шелковой занавеске или отгрызает полпаспорта. Где, скажите, теперь красота этой кошки? Или другой пример – с зазнобой сердца. Предмет романтичнейших грез наконец-то садится с вами на заветную скамейку в парке – закат, пение птах… И тут он силой своего обаяния с оглушительным плюхом обрушивает вокруг вас красоту и гармонию столетних дубов, тополей и прочего. Где, спрашивается, красота момента? \\nЕсли от сказки после того, как ее рассказали,...</td>\n",
              "      <td>О живительной пользе укорота при выборе между плохим и хорошим</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[юзефович, гражданская война, пепеляев, якутия]</td>\n",
              "      <td>Апокалиптический бунт</td>\n",
              "      <td>https://amp.ng.ru/?p=http://www.ng.ru/zavisimaya/2017-12-19/15_7139_bunt.html</td>\n",
              "      <td>Когда-то Леонид Юзефович написал книгу о монгольской эпопее барона Унгерна «Самодержец пустыни» – она стала интеллектуальным бестселлером и классикой жанра – документальный роман. В то время автор попутно изучал и историю вооруженного восстания в Якутии в 1922–1923 годах под руководством Анатолия Пепеляева. И вот теперь из «якутского» материала сложилась отдельная книга. Тема ее для нынешнего читателя поистине раритетна. Ведь воевавший где-то на самом краю страны Пепеляев практически забыт, притом что о борьбе с ним когда-то в СССР выходили статьи и книги. В памяти потомков, образно говоря, от Пепеляева остался только пепел.\\nЮзефович воскрешает в памяти не только его военные дела, но и человеческие черты. Этот провинциальный интеллигент, неврастеник и фаталист, начал восстание, практически не имея шансов на успех. Однако силою недюжинной харизмы Пепеляев сумел собрать вокруг себя многих боевых офицеров, таежных охотников и недовольных новыми порядками аборигенов. Для своих 32 лет ...</td>\n",
              "      <td>Крепость из тел и призрак независимой Якутии</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        keywords  ...                                                                                                                                summary\n",
              "0  [школа, образовательные стандарты, литература, история, фгос]  ...  Глава Минобрнауки считает, что в нездоровом ажиотаже вокруг новых образовательных стандартов виноваты издательства учебной литературы\n",
              "1                                              [красота, законы]  ...                                                                         О живительной пользе укорота при выборе между плохим и хорошим\n",
              "2                [юзефович, гражданская война, пепеляев, якутия]  ...                                                                                           Крепость из тел и призрак независимой Якутии\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RYcojH6ir3X"
      },
      "source": [
        "Каждой статье приписано какое-то количество ключевых слов. **Наша задача - придумать как извлекать точно такой же список автоматически.**\n",
        "Зададим несколько метрик, по которым будем определять качество извлекаемых ключевых слов - точность, полноту, ф1-меру и меру жаккарда."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYtfQFMyir3Z"
      },
      "source": [
        "def evaluate(true_kws, predicted_kws):\n",
        "    assert len(true_kws) == len(predicted_kws)\n",
        "    \n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1s = []\n",
        "    jaccards = []\n",
        "    \n",
        "    for i in range(len(true_kws)):\n",
        "        \n",
        "        true_kw = set(true_kws[i])\n",
        "        predicted_kw = set(predicted_kws[i])\n",
        "        \n",
        "        tp = len(true_kw & predicted_kw)\n",
        "        union = len(true_kw | predicted_kw)\n",
        "        fp = len(predicted_kw - true_kw)\n",
        "        fn = len(true_kw - predicted_kw)\n",
        "        \n",
        "        if (tp+fp) == 0:\n",
        "            prec = 0\n",
        "        else:\n",
        "            prec = tp / (tp + fp)\n",
        "        \n",
        "        if (tp+fn) == 0:\n",
        "            rec = 0\n",
        "        else:\n",
        "            rec = tp / (tp + fn)\n",
        "        if (prec+rec) == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = (2*(prec*rec))/(prec+rec)\n",
        "            \n",
        "        jac = tp / union\n",
        "        \n",
        "        precisions.append(prec)\n",
        "        recalls.append(rec)\n",
        "        f1s.append(f1)\n",
        "        jaccards.append(jac)\n",
        "    print('Precision - ', round(np.mean(precisions), 2))\n",
        "    print('Recall - ', round(np.mean(recalls), 2))\n",
        "    print('F1 - ', round(np.mean(f1s), 2))\n",
        "    print('Jaccard - ', round(np.mean(jaccards), 2))\n",
        "    \n",
        "    \n",
        "        "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2tWWSczir3b"
      },
      "source": [
        "Проверим, что всё работает как надо."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG_J6R-lir3b",
        "outputId": "5135bc3b-1673-4616-ed85-3199683d84ba"
      },
      "source": [
        "evaluate(data['keywords'], data['keywords'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  1.0\n",
            "Recall -  1.0\n",
            "F1 -  1.0\n",
            "Jaccard -  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFRO-ujsx00B"
      },
      "source": [
        "# Бейзлайн"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcvHq5C0ir5N"
      },
      "source": [
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ7ssNd9ir5s"
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
        "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
        "\n",
        "    return words"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjy5uKyhir5u"
      },
      "source": [
        "data['content_norm'] = data['content'].apply(normalize)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKWsRz_qy_n5"
      },
      "source": [
        "data['title_norm'] = data['title'].apply(normalize)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrB4445ir58"
      },
      "source": [
        "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA8UOm4jir58"
      },
      "source": [
        "# можно заодно сделать нграммы\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmYYKNJ7ir59",
        "outputId": "3cd610af-3e4a-438f-d289-328092ee3035"
      },
      "source": [
        "tfidf.fit(data['content_norm_str'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=2, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEhHPgpSir5-"
      },
      "source": [
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FfvoOEGir5_"
      },
      "source": [
        "Преобразуем наши тексты в векторы, где на позиции i стоит tfidf коэффициент слова i из словаря."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YXvKtxzir5_"
      },
      "source": [
        "texts_vectors = tfidf.transform(data['content_norm_str'])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFXbH6g-ir5_"
      },
      "source": [
        "Отсортируем векторы текстов по этим коэффициентам и возьмем топ-10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk648w31ir6A"
      },
      "source": [
        "## так как матрица в tfidf в спарс формате, ее нельзя просто так отсортировать\n",
        "## перевести ее в обычный формат для всех данных тоже не получится - не хватит памяти\n",
        "## поэтому пройдем по строчкам, переведем строчку в обычный array и отсортируем ее\n",
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords.append([id2word[w] for w in top_inds])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C74wdV8Gir6B",
        "outputId": "f4962ff4-dcf6-42e6-ecd8-262492cc2b4f"
      },
      "source": [
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.13\n",
            "Recall -  0.25\n",
            "F1 -  0.16\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYzBXN4lir6K"
      },
      "source": [
        "Возьмем этот результат за **baseline.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd9JRPegir6K"
      },
      "source": [
        "Precision -  0.13\n",
        "Recall -  0.25\n",
        "F1 -  0.16\n",
        "Jaccard -  0.09"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcKsaQwIfwPp"
      },
      "source": [
        "# Подходы к преодолению бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHPNWTDG5Q5p"
      },
      "source": [
        "Посмотрим еще раз на то, что бывает в оригинальных ключевых словах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiQDsVLg2n5Z",
        "outputId": "65da98a7-e6f3-42da-b3ca-5e128e27b8cb"
      },
      "source": [
        "list(data['keywords'])[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['школа', 'образовательные стандарты', 'литература', 'история', 'фгос'],\n",
              " ['красота', 'законы'],\n",
              " ['юзефович', 'гражданская война', 'пепеляев', 'якутия'],\n",
              " ['формула1', 'автоспорт', 'гонки', 'испания', 'квят'],\n",
              " ['есенин',\n",
              "  'православие',\n",
              "  'святая русь',\n",
              "  'поэзия',\n",
              "  'год литературы',\n",
              "  'клюев',\n",
              "  'мариенгоф',\n",
              "  'стихи',\n",
              "  'россия'],\n",
              " ['медвузы', 'медицинское образование', 'рудн', 'николай стуров', 'интервью'],\n",
              " ['литература',\n",
              "  'книги',\n",
              "  'периодика',\n",
              "  'космос',\n",
              "  'небо',\n",
              "  'астрономия',\n",
              "  'анатомия',\n",
              "  'филология'],\n",
              " ['сша', 'ирак', 'война'],\n",
              " ['искусственный интеллект', 'робот', 'компьютер', 'технологии'],\n",
              " ['вб', 'вто', 'переговоры', 'тарифы']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCUvcFpE9Trh"
      },
      "source": [
        "1. Можно отметить, что достаточно часто встречаются прилагательные. Попробуем их тоже оставлять при нормализации (и числительные тоже)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUmGDNQj2UDB"
      },
      "source": [
        "def normalize_2(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
        "    words = [word.normal_form for word in words if word.tag.POS in ['NOUN', 'ADJF', 'NUMR']]\n",
        "\n",
        "    return words"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVqiJ-1C3UcF"
      },
      "source": [
        "data['content_norm_2'] = data['content'].apply(normalize_2)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE10kxXd3cy0"
      },
      "source": [
        "data['title_norm_2'] = data['title'].apply(normalize_2)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTxoBmw_3d5l"
      },
      "source": [
        "data['content_norm_str_2'] = data['content_norm_2'].apply(' '.join)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnt5tc5y3fDX"
      },
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Tilpv33fKY"
      },
      "source": [
        "tfidf.fit(data['content_norm_str_2'])\n",
        "texts_vectors = tfidf.transform(data['content_norm_str_2'])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtiZsWRc4zYU"
      },
      "source": [
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9PQMjCp4DUu"
      },
      "source": [
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords.append([id2word[w] for w in top_inds])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OSRcMoh4HFe",
        "outputId": "3d240f13-852c-4966-ff34-d839760367fc"
      },
      "source": [
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.12\n",
            "Recall -  0.23\n",
            "F1 -  0.15\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8RTLhO77z2p"
      },
      "source": [
        "Не вышло :'D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-4nGVP077dU"
      },
      "source": [
        "2. Попробуем оставить только униграммы, так как, кажется, биграмм не так много. Также немного увеличим значение min_df (с 2 до 5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2OBMnm8XZn"
      },
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,1), min_df=5)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iNjmt818gct"
      },
      "source": [
        "tfidf.fit(data['content_norm_str'])\n",
        "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
        "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xzw7jNL8zKY"
      },
      "source": [
        "keywords = []\n",
        "\n",
        "for row in range(texts_vectors.shape[0]):\n",
        "    row_data = texts_vectors.getrow(row)\n",
        "    top_inds = row_data.toarray().argsort()[0,:-11:-1]\n",
        "    keywords.append([id2word[w] for w in top_inds])"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz36AMP281mB",
        "outputId": "88b76243-9579-44ba-d6b0-9e6d8909d220"
      },
      "source": [
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.14\n",
            "Recall -  0.26\n",
            "F1 -  0.17\n",
            "Jaccard -  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp0ygcrH9Eke"
      },
      "source": [
        "Бейзлайн побит."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn1CgKuf9lAL"
      },
      "source": [
        "3. Тут я пробовала поработать с параметрами gensim, но результат так и не побил бейзлайн"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQFndx779vcZ"
      },
      "source": [
        "from gensim.summarization import keywords"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pNL3eO1-ank",
        "outputId": "270fc95c-51ca-4f56-e31c-4edfbc6da662"
      },
      "source": [
        "gensim_kws = data['content_norm'].apply(lambda x: keywords(' '.join(x), pos_filter=['NN']).split('\\n')[:10])\n",
        "evaluate(data['keywords'], gensim_kws)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.07\n",
            "Recall -  0.11\n",
            "F1 -  0.08\n",
            "Jaccard -  0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AgV2EMQ-nVd"
      },
      "source": [
        "4. Rake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-LKCY07-pT2",
        "outputId": "03d35d33-41e3-4e24-8bbf-df650837b83e"
      },
      "source": [
        "!pip install rake-nltk"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7829 sha256=3662c5b0e2ef5bcfaf3fa443f5f3b86d9474152d3619c811973956000e39679f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxjcxlBp-8uD"
      },
      "source": [
        "from rake_nltk import Rake\n",
        "\n",
        "rake = Rake(language = 'russian', stopwords = stops, punctuations = punct)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh7boYMNHloO",
        "outputId": "a08cea7c-3252-48d3-98d5-1416379ac0d7"
      },
      "source": [
        "keywords = []\n",
        "\n",
        "for text in list(data['content_norm_str']):\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    #print(len(rake.get_ranked_phrases()))\n",
        "    kwords = [word for phrase in rake.get_ranked_phrases() for word in phrase.split(' ')]\n",
        "    c = Counter(kwords)\n",
        "    keywords.append([words for words, i in c.most_common(10)])\n",
        "\n",
        "evaluate(data['keywords'], keywords)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.13\n",
            "Recall -  0.26\n",
            "F1 -  0.17\n",
            "Jaccard -  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkRG9zKcFnfH"
      },
      "source": [
        "Бейзлайн побит."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMoGge_pG86e"
      },
      "source": [
        "Я пробовала задавать и другие параметры, например, max_length, но тогда результат сильно ушудшался."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1z-C4h-ir6L"
      },
      "source": [
        "5. Графы без networkx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIEAvFu6ir6O"
      },
      "source": [
        "from itertools import combinations"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQCj1FHBir6Z"
      },
      "source": [
        "def get_kws(text, top=5, window_size=5, random_p=0.1, len_check = False):\n",
        "\n",
        "    vocab = set(text)\n",
        "    word2id = {w:i for i, w in enumerate(vocab)}\n",
        "    id2word = {i:w for i, w in enumerate(vocab)}\n",
        "    # преобразуем слова в индексы для удобства\n",
        "    ids = [word2id[word] for word in text]\n",
        "\n",
        "    # создадим матрицу совстречаемости\n",
        "    m = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "    # пройдемся окном по всему тексту\n",
        "    for i in range(0, len(ids), window_size):\n",
        "        window = ids[i:i+window_size]\n",
        "        # добавим единичку всем парам слов в этом окне\n",
        "        for j, k in combinations(window, 2):\n",
        "            # чтобы граф был ненаправленный \n",
        "            m[j][k] += 1\n",
        "            m[k][j] += 1\n",
        "    \n",
        "    # нормализуем строки, чтобы получилась вероятность перехода\n",
        "    for i in range(m.shape[0]):\n",
        "        s = np.sum(m[i])\n",
        "        if not s:\n",
        "            continue\n",
        "        m[i] /= s\n",
        "    \n",
        "    # случайно выберем первое слова, а затем будет выбирать на основе полученых распределений\n",
        "    # сделаем так 5 раз и добавим каждое слово в счетчик\n",
        "    # чтобы не забиться в одном круге, иногда будет перескакивать на случайное слово\n",
        "    \n",
        "    c = Counter()\n",
        "    # начнем с абсолютного случайно выбранного элемента\n",
        "    n = np.random.choice(len(vocab))\n",
        "    for i in range(500): # если долго считается, можно уменьшить число проходов\n",
        "        \n",
        "        # c вероятностью random_p \n",
        "        # перескакиваем на другой узел\n",
        "        go_random = np.random.choice([0, 1], p=[1-random_p, random_p])\n",
        "        \n",
        "        if go_random:\n",
        "            n = np.random.choice(len(vocab))\n",
        "        \n",
        "        \n",
        "        ### \n",
        "        n = take_step(n, m)\n",
        "        # записываем узлы, в которых были\n",
        "        c.update([n])\n",
        "    \n",
        "    \n",
        "    if len_check == True:\n",
        "      counter_dict = {k:v for k, v in c.items() if len(id2word[k]) > 5 and id2word[k] not in stops}\n",
        "      c = Counter(counter_dict)\n",
        "    \n",
        "    # вернем топ-N\n",
        "    return [id2word[i] for i, count in c.most_common(top)]\n",
        "\n",
        "def take_step(n, matrix):\n",
        "    rang = len(matrix[n])\n",
        "    # выбираем узел из заданного интервала, на основе распределения из матрицы совстречаемости\n",
        "    if np.any(matrix[n]):\n",
        "        next_n = np.random.choice(range(rang), p=matrix[n])\n",
        "    else:\n",
        "        next_n = np.random.choice(range(rang))\n",
        "    return next_n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymiQu_g-ir6a",
        "outputId": "680be44a-8b2c-4d38-c962-80f3628ee994"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T64tLO5Sir6b",
        "outputId": "869eb912-39d7-4bfc-952a-03a6c8ddaded"
      },
      "source": [
        "%%time\n",
        "keywords_rw = data['content_norm'].progress_apply(lambda x: get_kws(x, 10, 10))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1987/1987 [02:41<00:00, 12.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 42s, sys: 14.4 s, total: 2min 57s\n",
            "Wall time: 2min 41s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le3-olLRMttX",
        "outputId": "4ac6b84e-3b28-4c5b-c661-f7a91570c431"
      },
      "source": [
        "evaluate(data['keywords'], keywords_rw)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.11\n",
            "Recall -  0.21\n",
            "F1 -  0.14\n",
            "Jaccard -  0.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj--HeGPQ3nW",
        "outputId": "3c4f4d44-a144-40a8-87f8-0cc64d10e2b3"
      },
      "source": [
        "%%time\n",
        "keywords_rw = data['content_norm'].progress_apply(lambda x: get_kws(x, top = 10, window_size = 30))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1987/1987 [02:48<00:00, 11.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 50s, sys: 13.3 s, total: 3min 3s\n",
            "Wall time: 2min 48s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v__A-KKHRG3Y",
        "outputId": "18f6059a-85f6-4e11-b845-212af22610c1"
      },
      "source": [
        "evaluate(data['keywords'], keywords_rw)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.12\n",
            "Recall -  0.23\n",
            "F1 -  0.15\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cARPh5pATEUf"
      },
      "source": [
        "Результат немного улучшился при увеличении окна с 10 до 30, однако безлайн побить все равно не удалось."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6hpJsLsXg7z"
      },
      "source": [
        "Еще я попробовала не считать слишком короткие слова и стоп-слова (кусочек с len_check), но результат стал хуже)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YvVouclXhS_"
      },
      "source": [
        "keywords_rw = data['content_norm'].progress_apply(lambda x: get_kws(x, top = 10, window_size = 30, len_check = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_ztVNk7VbOg",
        "outputId": "a770d920-bd9e-4679-aafa-d8e60fee294e"
      },
      "source": [
        "evaluate(data['keywords'], keywords_rw)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.09\n",
            "Recall -  0.18\n",
            "F1 -  0.12\n",
            "Jaccard -  0.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j6DTcqJRXLm"
      },
      "source": [
        "6. Графы с networkx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mcZ1lQvir6e"
      },
      "source": [
        "import networkx as nx"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b1TxFkBir6e"
      },
      "source": [
        "def build_matrix(text, window_size=5):\n",
        "    vocab = set(text)\n",
        "    word2id = {w:i for i, w in enumerate(vocab)}\n",
        "    id2word = {i:w for i, w in enumerate(vocab)}\n",
        "    # преобразуем слова в индексы для удобства\n",
        "    ids = [word2id[word] for word in text]\n",
        "\n",
        "    # создадим матрицу совстречаемости\n",
        "    m = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "    # пройдемся окном по всему тексту\n",
        "    for i in range(0, len(ids), window_size):\n",
        "        window = ids[i:i+window_size]\n",
        "        # добавим единичку всем парам слов в этом окне\n",
        "        for j, k in combinations(window, 2):\n",
        "            # чтобы граф был ненаправленный \n",
        "            m[j][k] += 1\n",
        "            m[k][j] += 1\n",
        "    \n",
        "    return m, id2word\n",
        "\n",
        "def some_centrality_measure(text, window_size=5, topn=5):\n",
        "    \n",
        "    matrix, id2word = build_matrix(text, window_size)\n",
        "    G = nx.from_numpy_array(matrix)\n",
        "    # тут можно поставить любую метрику\n",
        "    # менять тут \n",
        "    node2measure = dict(nx.pagerank_numpy(G)) #pagerank\n",
        "    \n",
        "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYojZvmKeF7u",
        "outputId": "06b108d9-6ad9-479d-ac92-dc2075ab5703"
      },
      "source": [
        "%%time\n",
        "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 10))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 42s, sys: 1min 17s, total: 2min 59s\n",
            "Wall time: 1min 32s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8SjMq5BeIiC",
        "outputId": "f1b7b4ad-55fc-4c7e-e626-5ce3dc89cc87"
      },
      "source": [
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.13\n",
            "Recall -  0.25\n",
            "F1 -  0.16\n",
            "Jaccard -  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nG1oZtbcAp4"
      },
      "source": [
        "Я уменьшила размер окна до 5: все метрики за исключением recall преодолели бейзлайн"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZP6zcBvYNGd"
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 5))"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anK-C_8Tir6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4d919c-0db3-4b24-95fe-b8e393599ff9"
      },
      "source": [
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.18\n",
            "Recall -  0.18\n",
            "F1 -  0.17\n",
            "Jaccard -  0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xibD2OQ5ctby"
      },
      "source": [
        "Если уменьшить до 2, то precision еще возрастет, а остальные метрики ухудшатся"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToJ2YgiBcPWj"
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 2))"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvc-h8c1cSP0",
        "outputId": "68a857b9-a417-4f69-f936-3d08c8b784c5"
      },
      "source": [
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.24\n",
            "Recall -  0.1\n",
            "F1 -  0.13\n",
            "Jaccard -  0.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxYJFwEgd3am"
      },
      "source": [
        "Если, наоборот, окно увеличить, тогда recall возрастет и побьет бейзлайн, но остальные метрики будут небольшими"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0_KOHq0c5Yb"
      },
      "source": [
        "keyword_nx = data['content_norm'].apply(lambda x: some_centrality_measure(x, 10, 15))"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqk3aa6Oc8nO",
        "outputId": "6fce3b97-7842-47d6-f05a-7cf5c3ea59ea"
      },
      "source": [
        "evaluate(data['keywords'], keyword_nx)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision -  0.1\n",
            "Recall -  0.29\n",
            "F1 -  0.14\n",
            "Jaccard -  0.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-M0idzEir7C"
      },
      "source": [
        "## Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lCI4o7yir7C"
      },
      "source": [
        "В семинаре использовался только небольшой кусочек данных. На всех данных пересчитайте baseline (tfidf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF5Gf48Lir7D"
      },
      "source": [
        "**Ваша задача - предложить 3 способа побить бейзлайн на всех данных.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGzVrtxHir7E"
      },
      "source": [
        "Нет никаких ограничений кроме:\n",
        "\n",
        "1) нельзя изменять метрику  \n",
        "2) решение должно быть воспроизводимым  \n",
        "3) способы дожны отличаться друг от друга не только гиперпараметрами (например, нельзя три раза поменять гиперпарамтры в TfidfVectorizer и сдать работу)  \n",
        "4) изменение количества извлекаемых слов не является улучшением (выберите одно значение и используйте только его)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AXCu2Sxir7E"
      },
      "source": [
        "В качестве ответа нужно предоставить jupyter тетрадку с экспериментами (обязательное условие!) и описать каждую из идей в форме - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4BWOr6air7F"
      },
      "source": [
        "Каждый реализованный и описанный способ оценивается в 3 балла. Дополнительный балл можно получить, если способы затрагивают разные аспекты решения (например, первая идея - улучшить нормализацию, вторая - улучшить способ представления текста в виде графа, третья - предложить способ удаления из топа идентичных ключевых слов (рф, россия))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG2TbRn3ir7O"
      },
      "source": [
        "Можно использовать мой код как основу, а можно придумать что-то полностью другое."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uGEhaOFir7O"
      },
      "source": [
        "Если у вас никак не получается побить бейзлайн вы можете предоставить реализацию и описание неудавшихся экспериментов (каждый оценивается в 1 балл)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIl2J2uYir7P"
      },
      "source": [
        "В поисках идей можно почитать обзоры по теме (посмотрите еще статьи, в которых цитируются эти обзоры): https://www.semanticscholar.org/search?year%5B0%5D=2012&year%5B1%5D=2020&publicationType%5B0%5D=Reviews&q=keyword%20extraction&sort=relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST0ehekHir7P"
      },
      "source": [
        "**Использовать доступные готовые решения тоже можно**. Так что погуглите перед тем, как приступать к заданию. "
      ]
    }
  ]
}