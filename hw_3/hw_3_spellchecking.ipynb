{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_3_spellchecking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmAFKzCkPk5T",
        "outputId": "66a24605-0ebe-4363-933d-f3aff74cfb84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #, force_remount=True"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqpzhICCPmff"
      },
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZcA8wbEQENf"
      },
      "source": [
        "!pip install pymorphy2\n",
        "!pip install pymystem3==0.1.10\n",
        "!pip install rusenttokenize\n",
        "!pip install razdel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCfpIzmYQBvb",
        "outputId": "f70653a9-66fc-4543-9f0a-867b55aab968"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZuVpwVUkJ40"
      },
      "source": [
        "**Задание 1.**\n",
        "\n",
        "Реализуйте алгоритм Symspell - https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам в словаре применяется только одна операция - удаление символа. Чтобы найти исправление из слова тоже удаляются символы и сравниваются с теми, что хранятся в словаре. Оцените качество полученного алгоритма теми же тремя метриками."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajkfScjUQK2D"
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = [word.text.strip(punctuation) for word \\\n",
        "                                                            in razdel_tokenize(text)]\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
        "    return normalized_text\n",
        "\n",
        "def preprocess(text):\n",
        "    sents = sentenize(text)\n",
        "    return [normalize(sent.text) for sent in sents]\n",
        "\n",
        "def ngrammer(tokens, n):\n",
        "    ngrams = []\n",
        "    tokens = [token for token in tokens]\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7ViD-yEQUmq"
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMXZpMoRQk1L"
      },
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FA5fhCrQphx",
        "outputId": "036ed80f-d63d-4f2b-a1b6-989778839066"
      },
      "source": [
        "pprint(align_words(true[1], bad[1])[:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('апофеозом', 'опофеозом'),\n",
            " ('дня', 'дня'),\n",
            " ('для', 'для'),\n",
            " ('меня', 'меня'),\n",
            " ('сегодня', 'сегодня')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1oqkg-xTz5m"
      },
      "source": [
        "corpus = open('wiki_data.txt', encoding='utf8').read()\n",
        "vocab = set(re.findall('\\w+', corpus.lower()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTbRWFNGUVYq"
      },
      "source": [
        "WORDS = Counter(vocab)\n",
        "N = sum(WORDS.values())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X6tgevrkoZ6"
      },
      "source": [
        "def predict_mistaken(word, vocab):\n",
        "    if word in vocab:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Dy9QAUj_gn",
        "outputId": "86217434-3e6b-441a-c3cb-9201b4dee111"
      },
      "source": [
        "def P(word, N=N): \n",
        "    \"Вычисляем вероятность слова\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "P('солнце')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.7114820418544366e-06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyXQucnctVKF"
      },
      "source": [
        "def edits1(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    return set(deletes)\n",
        "\n",
        "def dict_2(vocab):\n",
        "  \"Pre-calculation step\"\n",
        "  \"Generate terms with an edit distance from each dictionary term and add them together with the original term to the dictionary\"\n",
        "  #d = {}\n",
        "  d = defaultdict(list) #иначе ломается при неизвестных словах\n",
        "  for word in vocab:\n",
        "    edits = edits1(word)\n",
        "    for edit in edits:\n",
        "      if edit in d.keys():\n",
        "        d[edit].append(word)\n",
        "      else:\n",
        "        d[edit] = [word]\n",
        "  return d"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9zm-GpZtllu"
      },
      "source": [
        "err_dict = dict_2(WORDS)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcowYM8ZuoyJ",
        "outputId": "8690305b-dc6f-4d5c-f989-3d6177ed1da4"
      },
      "source": [
        "err_dict['облак']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['облаке', 'облако', 'облака']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2fpLrN-Wlzj"
      },
      "source": [
        "def candidates(word, d = err_dict):\n",
        "    \"Генерируем кандидатов на исправление\"\n",
        "    \"Generate terms with an edit distance from the input term and search them in the dictionary.\"\n",
        "    cands = d[word] #добавим кандидатов для изначального слова\n",
        "    cands.append(word) #добавим само слово\n",
        "    edits = edits1(word)\n",
        "    for i in edits:\n",
        "      for j in d[i]:\n",
        "        cands.append(j) #также добавим кандидатов для каждого варианта слова \n",
        "    return cands\n",
        "\n",
        "def correction(word): \n",
        "    \"Находим наиболее вероятное похожее слово\"\n",
        "    return max(candidates(word), key=P)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOBzO8Ajvx9E",
        "outputId": "2038c0eb-1ca7-4ae1-846a-2ec21d3d9720"
      },
      "source": [
        "len(candidates('облак'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dnwz9hK2vgyg",
        "outputId": "c9c42662-c961-4516-93b0-d18e8362487d"
      },
      "source": [
        "correction('облак')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'облаке'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk8ci4KgwDaq"
      },
      "source": [
        "Для оценки используем будем использовать три метрики:\n",
        "\n",
        "1) процент правильных слов;\n",
        "\n",
        "2) процент исправленных ошибок\n",
        "\n",
        "3) процент ошибочно исправленных правильных слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKKexZhmv6Zj",
        "outputId": "949a7f15-4840-4fb7-81ed-3da33aa22620"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
        "        # перед тем как считать исправление проверим нет ли его в кеше\n",
        "        # проверяем, что слова нет в словаре, чтобы не исправлять все слова\n",
        "        if not predict_mistaken(pair[1], WORDS):\n",
        "          predicted = pair[1]\n",
        "        else:\n",
        "          predicted = cashed.get(pair[1], correction(pair[1]))\n",
        "        \n",
        "        cashed[pair[1]] = predicted\n",
        "        \n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "\n",
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8643356643356643\n",
            "0.2985418265541059\n",
            "0.05099345354312622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1so-5_v78-g"
      },
      "source": [
        "**Задание 2.**\n",
        "\n",
        "Добавьте к полученному алгоритму исправления (Symspell) триграммную модель и проверьте, улучшает ли она качество. Триграммную модель нужно вставить туда, где у вас выбирается один из нескольких кандидатов на исправление."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKFvuaV3wZEQ"
      },
      "source": [
        "corpus_wiki = [['<start>', '<start>'] + sent + ['<end>'] for sent in preprocess(corpus)]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qr0Tgxb8BeJ"
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_wiki:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence, 2))\n",
        "    trigrams.update(ngrammer(sentence, 3))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83CCI2ntA6w7",
        "outputId": "97a29214-75ac-446b-dd7d-da147f8395e0"
      },
      "source": [
        "mistakes = []\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
        "    pred_sent = []\n",
        "    for j in range(2, len(word_pairs)):\n",
        "        pred = None\n",
        "        # проверяем, что слова нет в словаре, чтобы не исправлять все слова\n",
        "        if not predict_mistaken(word_pairs[j][1], WORDS):\n",
        "            pred = word_pairs[j][1]\n",
        "        else:\n",
        "            predicted = candidates(word_pairs[j][1]) # находим кандидатов для исправления\n",
        "            prev_word = word_pairs[j-2][1] + ' ' + word_pairs[j-1][1] # берем два предыдущих слова для контекста\n",
        "        \n",
        "            # если у нас нет в модели такой биграммы, берем первое по близости\n",
        "            if prev_word not in bigrams:\n",
        "                pred = predicted[0][0]\n",
        "            else:\n",
        "                lm_predicted = []\n",
        "                for word in predicted:\n",
        "                    trigram = ' '.join([prev_word, word])\n",
        "                    lm_predicted.append((word, (trigrams[trigram]/bigrams[prev_word]))) \n",
        "    \n",
        "                if lm_predicted:\n",
        "                  pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
        "        \n",
        "        if pred is None:\n",
        "            pred = word_pairs[j][1]\n",
        "        \n",
        "        if pred == word_pairs[j][0]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
        "        total += 1\n",
        "            \n",
        "        if word_pairs[j][0] == word_pairs[j][1]:\n",
        "            total_correct += 1\n",
        "            if word_pairs[j][0] !=  pred:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if word_pairs[j][0] == pred:\n",
        "                mistaken_fixed += 1\n",
        "\n",
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)                "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8343963052562129\n",
            "0.0026845637583892616\n",
            "0.09138819020241945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79FDw6uCKlO8"
      },
      "source": [
        "Получилось так, что результаты стали хуже по всем метрикам."
      ]
    }
  ]
}
